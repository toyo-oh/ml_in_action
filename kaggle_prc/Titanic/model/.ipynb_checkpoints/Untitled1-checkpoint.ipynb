{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3242eca45b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# 开始训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mtrain_data_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mtest_data_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    '''\n",
    "    要建立新的特征类目\n",
    "    1. Fare Category\n",
    "    2. Pclass Fare Category\n",
    "    3. Family Size Category\n",
    "    4. Age Group Category\n",
    "    5. Name Length Category\n",
    "    '''\n",
    "\n",
    "\n",
    "    # 弃掉不需要的列\n",
    "    def drop_col_not_req(df, cols):\n",
    "        df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 建立Fare Category\n",
    "    def fare_category(fare):\n",
    "        if fare <= 4:\n",
    "            return 'Very_Low_Fare'\n",
    "        elif fare <= 10:\n",
    "            return 'Low_Fare'\n",
    "        elif fare <= 30:\n",
    "            return 'Med_Fare'\n",
    "        elif fare <= 45:\n",
    "            return 'High_Fare'\n",
    "        else:\n",
    "            return 'Very_High_Fare'\n",
    "\n",
    "\n",
    "    # 建立PClass Fare Category\n",
    "    def pclass_fare_category(df, Pclass_1_mean_fare, Pclass_2_mean_fare, Pclass_3_mean_fare):\n",
    "        if df['Pclass'] == 1:\n",
    "            if df['Fare'] <= Pclass_1_mean_fare:\n",
    "                return 'Pclass_1_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_1_High_Fare'\n",
    "        elif df['Pclass'] == 2:\n",
    "            if df['Fare'] <= Pclass_2_mean_fare:\n",
    "                return 'Pclass_2_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_2_High_Fare'\n",
    "        elif df['Pclass'] == 3:\n",
    "            if df['Fare'] <= Pclass_3_mean_fare:\n",
    "                return 'Pclass_3_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_3_High_Fare'\n",
    "\n",
    "\n",
    "    # 建立Family Size Category\n",
    "    def family_size_category(family_size):\n",
    "        if family_size <= 1:\n",
    "            return 'Single'\n",
    "        elif family_size <= 3:\n",
    "            return 'Small_Family'\n",
    "        else:\n",
    "            return 'Large_Family'\n",
    "\n",
    "\n",
    "    # 建立Age_Group_category\n",
    "    def age_group_category(age):\n",
    "        if age <= 1:\n",
    "            return 'Baby'\n",
    "        elif age <= 4:\n",
    "            return 'Toddler'\n",
    "        elif age <= 12:\n",
    "            return 'Child'\n",
    "        elif age <= 19:\n",
    "            return 'Teenager'\n",
    "        elif age <= 30:\n",
    "            return 'Adult'\n",
    "        elif age <= 50:\n",
    "            return 'Middle_Aged'\n",
    "        elif age < 60:\n",
    "            return 'Senior_Citizen'\n",
    "        else:\n",
    "            return 'Old'\n",
    "\n",
    "\n",
    "    # 建立Name_Length_Category\n",
    "    def name_length_category(name_len):\n",
    "        if name_len <= 19:\n",
    "            return 'Very_Short_Name'\n",
    "        elif name_len <= 28:\n",
    "            return 'Short_Name'\n",
    "        elif name_len <= 45:\n",
    "            return 'Medium_Name'\n",
    "        else:\n",
    "            return 'Long_Name'\n",
    "\n",
    "\n",
    "    # 填充NaN值\n",
    "    # 使用GradientBoostingRegressor和LinearRegression来填充Age值\n",
    "    def fill_missing_age(missing_age_train, missing_age_test):\n",
    "        missing_age_X_train = missing_age_train.drop(['Age'], axis=1)\n",
    "        missing_age_Y_train = missing_age_train['Age']\n",
    "        missing_age_X_test = missing_age_test.drop(['Age'], axis=1)\n",
    "\n",
    "        gbm_reg = ensemble.GradientBoostingRegressor(random_state=42)\n",
    "        gbm_reg_param_grid = {'n_estimators': [2000], 'max_depth': [3],\n",
    "                              'learning_rate': [0.01], 'max_features': [3]}\n",
    "        gbm_reg_grid = model_selection.GridSearchCV(gbm_reg, gbm_reg_param_grid,\n",
    "                                                    cv=10, n_jobs=25, verbose=1,\n",
    "                                                    scoring='neg_mean_squared_error')\n",
    "        gbm_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "        print('Age feature Best GB Params:' + str(gbm_reg_grid.best_params_))\n",
    "        print('Age feature Best GB Score:' + str(gbm_reg_grid.best_score_))\n",
    "        print('GB Train Error for \"Age\" Feature Regressor:'+ str(gbm_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "        missing_age_test['Age_GB'] = gbm_reg_grid.predict(missing_age_X_test)\n",
    "        print(missing_age_test['Age_GB'][:4])\n",
    "\n",
    "        lrf_reg = LinearRegression()\n",
    "        lrf_reg_param_grid = {'fit_intercept': [True], 'normalize': [True]}\n",
    "        lrf_reg_grid = model_selection.GridSearchCV(lrf_reg, lrf_reg_param_grid,\n",
    "                                                    cv=10, n_jobs=25, verbose=1,\n",
    "                                                    scoring='neg_mean_squared_error')\n",
    "        lrf_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "        print('Age feature Best LR Params:' + str(lrf_reg_grid.best_params_))\n",
    "        print('Age feature Best LR Score:' + str(lrf_reg_grid.best_score_))\n",
    "        print('LR Train Error for \"Age\" Feature Regressor' + str(\n",
    "            lrf_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "\n",
    "        missing_age_test['Age_LRF'] = lrf_reg_grid.predict(missing_age_X_test)\n",
    "        print(missing_age_test['Age_LRF'][:4])\n",
    "\n",
    "        print('shape1',missing_age_test['Age'].shape,missing_age_test[['Age_GB','Age_LRF']].mode(axis=1).shape)\n",
    "        #missing_age_test['Age'] = missing_age_test[['Age_GB','Age_LRF']].mode(axis=1)\n",
    "        missing_age_test['Age'] = np.mean([missing_age_test['Age_GB'],missing_age_test['Age_LRF']])\n",
    "        print(missing_age_test['Age'][:4])\n",
    "        drop_col_not_req(missing_age_test, ['Age_GB', 'Age_LRF'])\n",
    "\n",
    "        return missing_age_test\n",
    "\n",
    "\n",
    "    # 筛选重要特征\n",
    "    def get_top_n_features(titanic_train_data_X, titanic_train_data_Y, top_n_features):\n",
    "        # 随机森林\n",
    "        rf_est = RandomForestClassifier(random_state=42)\n",
    "        rf_param_grid = {'n_estimators': [500], 'min_samples_split': [2, 3], 'max_depth': [20]}\n",
    "        rf_grid = model_selection.GridSearchCV(rf_est, rf_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        rf_grid.fit(titanic_train_data_X,titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best RF Params:' + str(rf_grid.best_params_))\n",
    "        print('Top N Features Best RF Score:' + str(rf_grid.best_score_))\n",
    "        print('Top N Features RF Train Error:' + str(rf_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_rf = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                              'importance': rf_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_rf = feature_imp_sorted_rf.head(top_n_features)['feature']\n",
    "        print('Sample 25 Features from RF Classifier')\n",
    "        print(str(features_top_n_rf[:25]))\n",
    "\n",
    "        # AdaBoost\n",
    "        ada_est = ensemble.AdaBoostClassifier(random_state=42)\n",
    "        ada_param_grid = {'n_estimators': [500], 'learning_rate': [0.5, 0.6]}\n",
    "        ada_grid = model_selection.GridSearchCV(ada_est, ada_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        ada_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best Ada Params:' + str(ada_grid.best_params_))\n",
    "        print('Top N Features Best Ada Score:' + str(ada_grid.best_score_))\n",
    "        print('Top N Features Ada Train Error:' + str(ada_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_ada = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                               'importance': ada_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_ada = feature_imp_sorted_ada.head(top_n_features)['feature']\n",
    "        print('Sample 25 Feature from Ada Classifier:')\n",
    "        print(str(features_top_n_ada[:25]))\n",
    "\n",
    "        # ExtraTree\n",
    "        et_est = ensemble.ExtraTreesClassifier(random_state=42)\n",
    "        et_param_grid = {'n_estimators': [500], 'min_samples_split': [3, 4], 'max_depth': [15]}\n",
    "        et_grid = model_selection.GridSearchCV(et_est, et_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        et_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best ET Params:' + str(et_grid.best_params_))\n",
    "        print('Top N Features Best ET Score:' + str(et_grid.best_score_))\n",
    "        print('Top N Features ET Train Error:' + str(et_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_et = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                              'importance': et_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_et = feature_imp_sorted_et.head(top_n_features)['feature']\n",
    "        print('Sample 25 Features from ET Classifier:')\n",
    "        print(str(features_top_n_et[:25]))\n",
    "\n",
    "        # 融合以上三个模型\n",
    "        features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et],\n",
    "                                   ignore_index=True).drop_duplicates()\n",
    "\n",
    "        return features_top_n\n",
    "\n",
    "\n",
    "    # 开始训练模型\n",
    "\n",
    "    train_data_org = pd.read_csv('../data/train.csv')\n",
    "    test_data_org = pd.read_csv('../data/test.csv')\n",
    "\n",
    "    # 将训练集和测试集合并\n",
    "\n",
    "    test_data_org['Survived'] = 0\n",
    "    combined_train_test = train_data_org.append(test_data_org)\n",
    "\n",
    "    # 特征工程\n",
    "\n",
    "    # 1. Embarked\n",
    "    # Embarkde中有两个NaN\n",
    "    if combined_train_test['Embarked'].isnull().sum() != 0:\n",
    "        combined_train_test['Embarked'].fillna(combined_train_test['Embarked'].mode().iloc[0], inplace=True)\n",
    "\n",
    "    emb_dummies_df = pd.get_dummies(combined_train_test['Embarked'],\n",
    "                                    prefix=combined_train_test[['Embarked']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, emb_dummies_df], axis=1)\n",
    "\n",
    "    # 2. Sex\n",
    "    sex_dummies_df = pd.get_dummies(combined_train_test['Sex'], prefix=combined_train_test[['Sex']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, sex_dummies_df], axis=1)\n",
    "\n",
    "    # 3. Name\n",
    "    # Title\n",
    "    combined_train_test['Title'] = combined_train_test['Name'].str.extract('.+,(.+)').str.extract(\n",
    "        '^(.+?)\\.').str.strip()\n",
    "    # print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "    title_Dict = {}\n",
    "    title_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\n",
    "    title_Dict.update(dict.fromkeys(['Jonkheer', 'Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\n",
    "    title_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\n",
    "    title_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\n",
    "    title_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\n",
    "    title_Dict.update(dict.fromkeys(['Master'], 'Master'))\n",
    "\n",
    "    combined_train_test['Title'] = combined_train_test['Title'].map(title_Dict)\n",
    "    # print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "    title_dummies_df = pd.get_dummies(combined_train_test['Title'], prefix=combined_train_test[['Title']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, title_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Name Length\n",
    "    combined_train_test['Name_Length'] = combined_train_test['Name'].str.len()\n",
    "    combined_train_test['Name_length_Category'] = combined_train_test['Name_Length'].map(name_length_category)\n",
    "    # print(combined_train_test['Name_length_Category'].groupby(by=combined_train_test['Name_length_Category']).count().sort_values(ascending=False))\n",
    "\n",
    "    le_name = LabelEncoder()\n",
    "    le_name.fit(np.array(['Very_Short_Name', 'Short_Name', 'Medium_Name', 'Long_Name']))\n",
    "    combined_train_test['Name_length_Category'] = le_name.transform(combined_train_test['Name_length_Category'])\n",
    "\n",
    "    # print(combined_train_test[['Name_length_Category','Survived']].corr())\n",
    "    name_length_dummies_df = pd.get_dummies(combined_train_test['Name_length_Category'],\n",
    "                                            prefix=combined_train_test[['Name_length_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, name_length_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # First Name\n",
    "    combined_train_test['First_Name'] = combined_train_test['Name'].str.extract('^(.+?),').str.strip()\n",
    "    first_name_dummies_df = pd.get_dummies(combined_train_test['First_Name'],\n",
    "                                           prefix=combined_train_test[['First_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, first_name_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Last_Name\n",
    "    combined_train_test['Last_Name'] = combined_train_test['Name'].str.split('\\.').str[1].str.strip()\n",
    "    combined_train_test['Last_Name'] = combined_train_test['Last_Name'].str.strip('\\([^)]*\\)')\n",
    "    combined_train_test['Last_Name'].fillna(combined_train_test['Name'].str.split('\\.').str[1].str.strip())\n",
    "    # print(combined_train_test['Last_Name'].groupby(by = combined_train_test['Last_Name']).count().sort_values(ascending = False)[:5])\n",
    "    last_name_dummies_df = pd.get_dummies(combined_train_test['Last_Name'],\n",
    "                                          prefix=combined_train_test[['Last_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, last_name_dummies_df], axis=1)\n",
    "\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Original_Name\n",
    "    combined_train_test['Original_Name'] = combined_train_test['Name'].str.split('\\((.*?)\\)').str[1].str.strip(\n",
    "        '\\\"').str.strip()\n",
    "    # print(combined_train_test['Original_Name'].groupby(by = combined_train_test['Original_Name']).count().sort_values(ascending = False)[:5])\n",
    "    original_name_dummies_df = pd.get_dummies(combined_train_test['Original_Name'],\n",
    "                                              prefix=combined_train_test[['Original_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, original_name_dummies_df], axis=1)\n",
    "\n",
    "    # 4. Fare\n",
    "    # 填充NaN\n",
    "    if combined_train_test['Fare'].isnull().sum() != 0:\n",
    "        combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "            combined_train_test.groupby('Pclass').transform('mean'))\n",
    "\n",
    "    # 将多人船票的价格平均到每人\n",
    "    combined_train_test['Group_Ticket'] = combined_train_test['Fare'].groupby(\n",
    "        by=combined_train_test['Ticket']).transform('count')\n",
    "    combined_train_test['Fare'] = combined_train_test['Fare'] / combined_train_test['Group_Ticket']\n",
    "    combined_train_test.drop(['Group_Ticket'], axis=1, inplace=True)\n",
    "\n",
    "    # 去除Fare为0的项\n",
    "    if sum(n == 0 for n in combined_train_test.Fare.values.flatten()) > 0:\n",
    "        combined_train_test.loc[combined_train_test.Fare == 0, 'Fare'] = np.nan\n",
    "        combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "            combined_train_test.groupby('Pclass').transform('mean'))\n",
    "    # print(combined_train_test['Fare'].describe())\n",
    "    # 建立Fare Category\n",
    "    combined_train_test['Fare_Category'] = combined_train_test['Fare'].map(fare_category)\n",
    "    le_fare = LabelEncoder()\n",
    "    le_fare.fit(np.array(['Very_Low_Fare', 'Low_Fare', 'Med_Fare', 'High_Fare', 'Very_High_Fare']))\n",
    "    combined_train_test['Fare_Category'] = le_fare.transform(combined_train_test['Fare_Category'])\n",
    "\n",
    "    fare_cat_dummies_df = pd.get_dummies(combined_train_test['Fare_Category'],\n",
    "                                         prefix=combined_train_test[['Fare_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, fare_cat_dummies_df], axis=1)\n",
    "    # print(combined_train_test['Fare_Category'].groupby(by = combined_train_test['Fare_Category']).count().sort_values(ascending = False))\n",
    "\n",
    "    # 5. Pclass\n",
    "    # print(combined_train_test['Fare'].groupby(by = combined_train_test['Pclass']).mean())\n",
    "    Pclass_1_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([1]).values[0]\n",
    "    Pclass_2_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([2]).values[0]\n",
    "    Pclass_3_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([3]).values[0]\n",
    "    # 建立Pclass_Fare Category\n",
    "    combined_train_test['Pclass_Fare_Category'] = combined_train_test.apply(pclass_fare_category, args=(\n",
    "    Pclass_1_mean_fare, Pclass_2_mean_fare, Pclass_3_mean_fare), axis=1)\n",
    "    # print(combined_train_test['Pclass_Fare_Category'].groupby(by = combined_train_test['Pclass_Fare_Category']).count().sort_values(ascending = False))\n",
    "    p_fare = LabelEncoder()\n",
    "    p_fare.fit(np.array(\n",
    "        ['Pclass_1_Low_Fare', 'Pclass_1_High_Fare', 'Pclass_2_Low_Fare', 'Pclass_2_High_Fare', 'Pclass_3_Low_Fare',\n",
    "         'Pclass_3_High_Fare']))\n",
    "    combined_train_test['Pclass_Fare_Category'] = p_fare.transform(combined_train_test['Pclass_Fare_Category'])\n",
    "\n",
    "    # 6. Parch and SibSp\n",
    "\n",
    "    combined_train_test['Family_Size'] = combined_train_test['Parch'] + combined_train_test['SibSp'] + 1\n",
    "    combined_train_test['Family_Size_Category'] = combined_train_test['Family_Size'].map(family_size_category)\n",
    "\n",
    "    le_family = LabelEncoder()\n",
    "    le_family.fit(np.array(['Single', 'Small_Family', 'Large_Family']))\n",
    "    combined_train_test['Family_Size_Category'] = le_family.transform(combined_train_test['Family_Size_Category'])\n",
    "\n",
    "    fam_size_cat_dummies_df = pd.get_dummies(combined_train_test['Family_Size_Category'],\n",
    "                                             prefix=combined_train_test[['Family_Size_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, fam_size_cat_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # 7. Age\n",
    "    # 填充Age中的NaN值\n",
    "\n",
    "    combined_train_test['Age_Null'] = combined_train_test['Age'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "\n",
    "    missing_age_df = pd.DataFrame(\n",
    "        combined_train_test[['Age', 'Parch', 'Sex', 'SibSp', 'Family_Size', 'Family_Size_Category',\n",
    "                             'Title', 'Fare', 'Fare_Category', 'Pclass', 'Embarked']])\n",
    "    missing_age_df = pd.get_dummies(missing_age_df,\n",
    "                                    columns=['Title', 'Family_Size_Category', 'Fare_Category', 'Sex', 'Pclass',\n",
    "                                             'Embarked'])\n",
    "    missing_age_train = missing_age_df[missing_age_df['Age'].notnull()]\n",
    "    missing_age_test = missing_age_df[missing_age_df['Age'].isnull()]\n",
    "    combined_train_test.loc[(combined_train_test.Age.isnull()), 'Age'] = fill_missing_age(missing_age_train,\n",
    "                                                                                          missing_age_test)\n",
    "    #print(combined_train_test.describe())\n",
    "\n",
    "    #检查是否有异常值\n",
    "    if sum(n<0 for n in combined_train_test.Age.values.flatten()) > 0:\n",
    "        combined_train_test.loc[combined_train_test.Age < 0,'Age'] = np.nan\n",
    "        combined_train_test['Age'] = combined_train_test[['Age']].fillna(combined_train_test.groupby('Title').transform('mean'))\n",
    "    #print(combined_train_test['Age'].groupby(by=combined_train_test['Title']).mean().sort_values(ascending=True))\n",
    "\n",
    "    # 建立Age_Category\n",
    "    combined_train_test['Age_Category'] = combined_train_test['Age'].map(age_group_category)\n",
    "    le_age = LabelEncoder()\n",
    "    le_age.fit(np.array(['Baby','Toddler','Child','Teenager','Adult','Middle_Aged','Senior_Citizen','Old']))\n",
    "    combined_train_test['Age_Category'] = le_age.transform(combined_train_test['Age_Category'])\n",
    "    age_cat_dummies_df = pd.get_dummies(combined_train_test['Age_Category'],\n",
    "                                        prefix=combined_train_test[['Age_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test,age_cat_dummies_df],axis=1)\n",
    "\n",
    "    #8. Ticket\n",
    "    combined_train_test['Ticket_Letter'] = combined_train_test['Ticket'].str.split().str[0]\n",
    "    combined_train_test['Ticket_Letter'] = combined_train_test['Ticket_Letter'].apply(lambda x:np.nan if x.isnumeric() else x)\n",
    "    combined_train_test['Ticket_Number'] = combined_train_test['Ticket'].apply(lambda x: pd.to_numeric(x,errors='coerce'))\n",
    "    combined_train_test['Ticket_Number'].fillna(0,inplace=True)\n",
    "    combined_train_test = pd.get_dummies(combined_train_test,columns=['Ticket','Ticket_Letter'])\n",
    "    #print(combined_train_test.shape)\n",
    "\n",
    "    #9. Cabin\n",
    "    combined_train_test['Cabin_Letter'] = combined_train_test['Cabin'].apply(lambda x:str(x)[0] if pd.notnull(x) else x)\n",
    "    combined_train_test = pd.get_dummies(combined_train_test,columns=['Cabin','Cabin_Letter'])\n",
    "    #print(combined_train_test.shape)\n",
    "\n",
    "    #10. 将Age和Fare正则化\n",
    "    scale_age_fare = preprocessing.StandardScaler().fit(combined_train_test[['Age','Fare']])\n",
    "    combined_train_test[['Age','Fare']] = scale_age_fare.transform(combined_train_test[['Age','Fare']])\n",
    "\n",
    "    #11. 弃掉无用列\n",
    "    combined_train_test.drop(['Name', 'PassengerId', 'Embarked', 'Sex', 'Title', 'Fare_Category',\n",
    "                              'Family_Size_Category', 'Age_Category', 'First_Name', 'Last_Name',\n",
    "                              'Original_Name', 'Name_length_Category'],axis=1,inplace=True)\n",
    "    print(combined_train_test.describe())\n",
    "    #12. 整理数据\n",
    "\n",
    "    train_data = combined_train_test[:891]\n",
    "    test_data = combined_train_test[891:]\n",
    "\n",
    "    titanic_train_data_X = train_data.drop(['Survived'],axis=1)\n",
    "    titanic_train_data_Y = train_data['Survived']\n",
    "\n",
    "    titanic_test_data_X = test_data.drop(['Survived'],axis=1)\n",
    "\n",
    "    #13. 利用特征值重要性排名来去除无用列\n",
    "    feature_to_pick = 250\n",
    "    feature_top_n = get_top_n_features(titanic_train_data_X,titanic_train_data_Y,feature_to_pick)\n",
    "    print('Total Feature:'+str(combined_train_test.shape))\n",
    "    print('Picked Feature'+str(feature_top_n.shape))\n",
    "\n",
    "    titanic_train_data_X = titanic_train_data_X[feature_top_n]\n",
    "    del titanic_train_data_X['Ticket_Number']\n",
    "    titanic_test_data_X = titanic_test_data_X[feature_top_n]\n",
    "    del titanic_test_data_X['Ticket_Number']\n",
    "\n",
    "    #14.建立模型\n",
    "    rf_est = ensemble.RandomForestClassifier(n_estimators = 750, criterion = 'gini', max_features = 'sqrt',\n",
    "                                             max_depth = 3, min_samples_split = 4, min_samples_leaf = 2,\n",
    "                                             n_jobs = 50, random_state = 42, verbose = 1)\n",
    "    gbm_est = ensemble.GradientBoostingClassifier(n_estimators=900, learning_rate=0.0008, loss='exponential',\n",
    "                                                  min_samples_split=3, min_samples_leaf=2, max_features='sqrt',\n",
    "                                                  max_depth=3, random_state=42, verbose=1)\n",
    "    et_est = ensemble.ExtraTreesClassifier(n_estimators=750, max_features='sqrt', max_depth=35, n_jobs=50,\n",
    "                                           criterion='entropy', random_state=42, verbose=1)\n",
    "\n",
    "    voting_est = ensemble.VotingClassifier(estimators = [('rf', rf_est),('gbm', gbm_est),('et', et_est)],\n",
    "                                       voting = 'soft', weights = [3,5,2],\n",
    "                                       n_jobs = 50)\n",
    "    voting_est.fit(titanic_train_data_X,titanic_train_data_Y)\n",
    "    print('VotingClassifier Score:' + str(voting_est.score(titanic_train_data_X,titanic_train_data_Y)))\n",
    "    print('VotingClassifier Estimators:' + str(voting_est.estimators_))\n",
    "\n",
    "    #预测\n",
    "    titanic_test_data_X['Survived'] = voting_est.predict(titanic_test_data_X)\n",
    "\n",
    "    submission = pd.DataFrame({'PassengerId':test_data_org.loc[:,'PassengerId'],\n",
    "                               'Survived':titanic_test_data_X.loc[:,'Survived']})\n",
    "\n",
    "    submission.to_csv('submission_result.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
