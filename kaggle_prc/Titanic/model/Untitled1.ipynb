{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:241: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:242: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:276: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Name_length_Category  Survived\n",
      "Name_length_Category              1.000000 -0.229908\n",
      "Survived                         -0.229908  1.000000\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Done   5 out of  10 | elapsed:    2.4s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age feature Best GB Params:{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 3, 'n_estimators': 2000}\n",
      "Age feature Best GB Score:-112.554953793\n",
      "GB Train Error for \"Age\" Feature Regressor:-91.3322555947\n",
      "5     33.501856\n",
      "17    33.285687\n",
      "19    33.233397\n",
      "26    26.619434\n",
      "Name: Age_GB, dtype: float64\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age feature Best LR Params:{'fit_intercept': True, 'normalize': True}\n",
      "Age feature Best LR Score:-119.634403653\n",
      "LR Train Error for \"Age\" Feature Regressor-114.494213729\n",
      "5     34.6250\n",
      "17    33.7500\n",
      "19    31.0625\n",
      "26    26.9375\n",
      "Name: Age_LRF, dtype: float64\n",
      "shape1 (263,) (263, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:142: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5     29.58415\n",
      "17    29.58415\n",
      "19    29.58415\n",
      "26    29.58415\n",
      "Name: Age, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Age          Fare        Parch       Pclass        SibSp  \\\n",
      "count  1.309000e+03  1.309000e+03  1309.000000  1309.000000  1309.000000   \n",
      "mean   2.527899e-16 -7.667239e-17     0.385027     2.294882     0.498854   \n",
      "std    1.000382e+00  1.000382e+00     0.865560     0.837836     1.041658   \n",
      "min   -2.302343e+00 -8.759798e-01     0.000000     1.000000     0.000000   \n",
      "25%   -6.073124e-01 -5.435530e-01     0.000000     2.000000     0.000000   \n",
      "50%   -1.842698e-02 -4.967240e-01     0.000000     3.000000     0.000000   \n",
      "75%    4.020967e-01  2.374692e-03     0.000000     3.000000     1.000000   \n",
      "max    3.896205e+00  8.360045e+00     9.000000     3.000000     8.000000   \n",
      "\n",
      "          Survived   Embarked_C   Embarked_Q   Embarked_S   Sex_female  \\\n",
      "count  1309.000000  1309.000000  1309.000000  1309.000000  1309.000000   \n",
      "mean      0.261268     0.206264     0.093965     0.699771     0.355997   \n",
      "std       0.439494     0.404777     0.291891     0.458533     0.478997   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     1.000000     0.000000   \n",
      "75%       1.000000     0.000000     0.000000     1.000000     1.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "            ...           Cabin_G6      Cabin_T  Cabin_Letter_A  \\\n",
      "count       ...        1309.000000  1309.000000     1309.000000   \n",
      "mean        ...           0.003820     0.000764        0.016807   \n",
      "std         ...           0.061709     0.027639        0.128596   \n",
      "min         ...           0.000000     0.000000        0.000000   \n",
      "25%         ...           0.000000     0.000000        0.000000   \n",
      "50%         ...           0.000000     0.000000        0.000000   \n",
      "75%         ...           0.000000     0.000000        0.000000   \n",
      "max         ...           1.000000     1.000000        1.000000   \n",
      "\n",
      "       Cabin_Letter_B  Cabin_Letter_C  Cabin_Letter_D  Cabin_Letter_E  \\\n",
      "count     1309.000000     1309.000000     1309.000000     1309.000000   \n",
      "mean         0.049656        0.071811        0.035141        0.031322   \n",
      "std          0.217317        0.258273        0.184207        0.174252   \n",
      "min          0.000000        0.000000        0.000000        0.000000   \n",
      "25%          0.000000        0.000000        0.000000        0.000000   \n",
      "50%          0.000000        0.000000        0.000000        0.000000   \n",
      "75%          0.000000        0.000000        0.000000        0.000000   \n",
      "max          1.000000        1.000000        1.000000        1.000000   \n",
      "\n",
      "       Cabin_Letter_F  Cabin_Letter_G  Cabin_Letter_T  \n",
      "count     1309.000000     1309.000000     1309.000000  \n",
      "mean         0.016043        0.003820        0.000764  \n",
      "std          0.125688        0.061709        0.027639  \n",
      "min          0.000000        0.000000        0.000000  \n",
      "25%          0.000000        0.000000        0.000000  \n",
      "50%          0.000000        0.000000        0.000000  \n",
      "75%          0.000000        0.000000        0.000000  \n",
      "max          1.000000        1.000000        1.000000  \n",
      "\n",
      "[8 rows x 3434 columns]\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:   17.5s remaining:    9.4s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best RF Params:{'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Top N Features Best RF Score:0.835016835017\n",
      "Top N Features RF Train Error:0.936026936027\n",
      "Sample 25 Features from RF Classifier\n",
      "9                     Sex_male\n",
      "8                   Sex_female\n",
      "12                    Title_Mr\n",
      "16                 Name_Length\n",
      "1                         Fare\n",
      "2245      Pclass_Fare_Category\n",
      "11                  Title_Miss\n",
      "13                   Title_Mrs\n",
      "3                       Pclass\n",
      "0                          Age\n",
      "2259             Ticket_Number\n",
      "2246               Family_Size\n",
      "2241           Fare_Category_1\n",
      "17      Name_length_Category_0\n",
      "2249    Family_Size_Category_2\n",
      "4                        SibSp\n",
      "2                        Parch\n",
      "18      Name_length_Category_1\n",
      "2248    Family_Size_Category_1\n",
      "7                   Embarked_S\n",
      "20      Name_length_Category_3\n",
      "5                   Embarked_C\n",
      "2242           Fare_Category_2\n",
      "19      Name_length_Category_2\n",
      "3429            Cabin_Letter_E\n",
      "Name: feature, dtype: object\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:   53.8s remaining:   29.0s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:   53.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best Ada Params:{'learning_rate': 0.5, 'n_estimators': 500}\n",
      "Top N Features Best Ada Score:0.843995510662\n",
      "Top N Features Ada Train Error:1.0\n",
      "Sample 25 Feature from Ada Classifier:\n",
      "2259             Ticket_Number\n",
      "1                         Fare\n",
      "9                     Sex_male\n",
      "8                   Sex_female\n",
      "16                 Name_Length\n",
      "0                          Age\n",
      "2364               Ticket_1601\n",
      "10                Title_Master\n",
      "12                    Title_Mr\n",
      "7                   Embarked_S\n",
      "3231      Ticket_Letter_STON/O\n",
      "3                       Pclass\n",
      "14               Title_Officer\n",
      "2245      Pclass_Fare_Category\n",
      "2246               Family_Size\n",
      "2249    Family_Size_Category_2\n",
      "3235       Ticket_Letter_W./C.\n",
      "13                   Title_Mrs\n",
      "3318             Cabin_C22 C26\n",
      "409         First_Name_Jussila\n",
      "54          First_Name_Asplund\n",
      "2257            Age_Category_6\n",
      "2247    Family_Size_Category_0\n",
      "37          First_Name_Allison\n",
      "74          First_Name_Barbara\n",
      "Name: feature, dtype: object\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:   16.0s remaining:    8.6s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best ET Params:{'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "Top N Features Best ET Score:0.832772166105\n",
      "Top N Features ET Train Error:0.918069584736\n",
      "Sample 25 Features from ET Classifier:\n",
      "8                   Sex_female\n",
      "12                    Title_Mr\n",
      "9                     Sex_male\n",
      "11                  Title_Miss\n",
      "13                   Title_Mrs\n",
      "3                       Pclass\n",
      "2241           Fare_Category_1\n",
      "2245      Pclass_Fare_Category\n",
      "2249    Family_Size_Category_2\n",
      "17      Name_length_Category_0\n",
      "16                 Name_Length\n",
      "18      Name_length_Category_1\n",
      "2248    Family_Size_Category_1\n",
      "1                         Fare\n",
      "2242           Fare_Category_2\n",
      "20      Name_length_Category_3\n",
      "7                   Embarked_S\n",
      "3426            Cabin_Letter_B\n",
      "19      Name_length_Category_2\n",
      "3429            Cabin_Letter_E\n",
      "2246               Family_Size\n",
      "5                   Embarked_C\n",
      "3428            Cabin_Letter_D\n",
      "3211          Ticket_Letter_PC\n",
      "2252            Age_Category_1\n",
      "Name: feature, dtype: object\n",
      "Total Feature:(1309, 3434)\n",
      "Picked Feature(439,)\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9726            3.75s\n",
      "         2           0.9725            3.76s\n",
      "         3           0.9722            3.58s\n",
      "         4           0.9720            3.62s\n",
      "         5           0.9719            3.88s\n",
      "         6           0.9718            5.04s\n",
      "         7           0.9717            4.87s\n",
      "         8           0.9715            4.62s\n",
      "         9           0.9713            4.48s\n",
      "        10           0.9710            4.31s\n",
      "        20           0.9694            2.99s\n",
      "        30           0.9679            2.45s\n",
      "        40           0.9669            2.28s\n",
      "        50           0.9658            2.10s\n",
      "        60           0.9641            1.98s\n",
      "        70           0.9627            1.89s\n",
      "        80           0.9614            1.79s\n",
      "        90           0.9601            1.73s\n",
      "       100           0.9591            1.69s\n",
      "       200           0.9445            1.16s\n",
      "       300           0.9313            0.92s\n",
      "       400           0.9193            0.72s\n",
      "       500           0.9075            0.55s\n",
      "       600           0.8952            0.40s\n",
      "       700           0.8828            0.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       800           0.8727            0.18s\n",
      "       900           0.8641            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier Score:0.92480359147\n",
      "VotingClassifier Estimators:[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=50,\n",
      "            oob_score=False, random_state=42, verbose=1, warm_start=False), GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.0008, loss='exponential', max_depth=3,\n",
      "              max_features='sqrt', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=2, min_samples_split=3,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=900,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=1,\n",
      "              warm_start=False), ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=50,\n",
      "           oob_score=False, random_state=42, verbose=1, warm_start=False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    '''\n",
    "    要建立新的特征类目\n",
    "    1. Fare Category\n",
    "    2. Pclass Fare Category\n",
    "    3. Family Size Category\n",
    "    4. Age Group Category\n",
    "    5. Name Length Category\n",
    "    '''\n",
    "\n",
    "\n",
    "    # 弃掉不需要的列\n",
    "    def drop_col_not_req(df, cols):\n",
    "        df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 建立Fare Category\n",
    "    def fare_category(fare):\n",
    "        if fare <= 4:\n",
    "            return 'Very_Low_Fare'\n",
    "        elif fare <= 10:\n",
    "            return 'Low_Fare'\n",
    "        elif fare <= 30:\n",
    "            return 'Med_Fare'\n",
    "        elif fare <= 45:\n",
    "            return 'High_Fare'\n",
    "        else:\n",
    "            return 'Very_High_Fare'\n",
    "\n",
    "\n",
    "    # 建立PClass Fare Category\n",
    "    def pclass_fare_category(df, Pclass_1_mean_fare, Pclass_2_mean_fare, Pclass_3_mean_fare):\n",
    "        if df['Pclass'] == 1:\n",
    "            if df['Fare'] <= Pclass_1_mean_fare:\n",
    "                return 'Pclass_1_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_1_High_Fare'\n",
    "        elif df['Pclass'] == 2:\n",
    "            if df['Fare'] <= Pclass_2_mean_fare:\n",
    "                return 'Pclass_2_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_2_High_Fare'\n",
    "        elif df['Pclass'] == 3:\n",
    "            if df['Fare'] <= Pclass_3_mean_fare:\n",
    "                return 'Pclass_3_Low_Fare'\n",
    "            else:\n",
    "                return 'Pclass_3_High_Fare'\n",
    "\n",
    "\n",
    "    # 建立Family Size Category\n",
    "    def family_size_category(family_size):\n",
    "        if family_size <= 1:\n",
    "            return 'Single'\n",
    "        elif family_size <= 3:\n",
    "            return 'Small_Family'\n",
    "        else:\n",
    "            return 'Large_Family'\n",
    "\n",
    "\n",
    "    # 建立Age_Group_category\n",
    "    def age_group_category(age):\n",
    "        if age <= 1:\n",
    "            return 'Baby'\n",
    "        elif age <= 4:\n",
    "            return 'Toddler'\n",
    "        elif age <= 12:\n",
    "            return 'Child'\n",
    "        elif age <= 19:\n",
    "            return 'Teenager'\n",
    "        elif age <= 30:\n",
    "            return 'Adult'\n",
    "        elif age <= 50:\n",
    "            return 'Middle_Aged'\n",
    "        elif age < 60:\n",
    "            return 'Senior_Citizen'\n",
    "        else:\n",
    "            return 'Old'\n",
    "\n",
    "\n",
    "    # 建立Name_Length_Category\n",
    "    def name_length_category(name_len):\n",
    "        if name_len <= 19:\n",
    "            return 'Very_Short_Name'\n",
    "        elif name_len <= 28:\n",
    "            return 'Short_Name'\n",
    "        elif name_len <= 45:\n",
    "            return 'Medium_Name'\n",
    "        else:\n",
    "            return 'Long_Name'\n",
    "\n",
    "\n",
    "    # 填充NaN值\n",
    "    # 使用GradientBoostingRegressor和LinearRegression来填充Age值\n",
    "    def fill_missing_age(missing_age_train, missing_age_test):\n",
    "        missing_age_X_train = missing_age_train.drop(['Age'], axis=1)\n",
    "        missing_age_Y_train = missing_age_train['Age']\n",
    "        missing_age_X_test = missing_age_test.drop(['Age'], axis=1)\n",
    "\n",
    "        gbm_reg = ensemble.GradientBoostingRegressor(random_state=42)\n",
    "        gbm_reg_param_grid = {'n_estimators': [2000], 'max_depth': [3],\n",
    "                              'learning_rate': [0.01], 'max_features': [3]}\n",
    "        gbm_reg_grid = model_selection.GridSearchCV(gbm_reg, gbm_reg_param_grid,\n",
    "                                                    cv=10, n_jobs=25, verbose=1,\n",
    "                                                    scoring='neg_mean_squared_error')\n",
    "        gbm_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "        print('Age feature Best GB Params:' + str(gbm_reg_grid.best_params_))\n",
    "        print('Age feature Best GB Score:' + str(gbm_reg_grid.best_score_))\n",
    "        print('GB Train Error for \"Age\" Feature Regressor:'+ str(gbm_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "        missing_age_test['Age_GB'] = gbm_reg_grid.predict(missing_age_X_test)\n",
    "        print(missing_age_test['Age_GB'][:4])\n",
    "\n",
    "        lrf_reg = LinearRegression()\n",
    "        lrf_reg_param_grid = {'fit_intercept': [True], 'normalize': [True]}\n",
    "        lrf_reg_grid = model_selection.GridSearchCV(lrf_reg, lrf_reg_param_grid,\n",
    "                                                    cv=10, n_jobs=25, verbose=1,\n",
    "                                                    scoring='neg_mean_squared_error')\n",
    "        lrf_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "        print('Age feature Best LR Params:' + str(lrf_reg_grid.best_params_))\n",
    "        print('Age feature Best LR Score:' + str(lrf_reg_grid.best_score_))\n",
    "        print('LR Train Error for \"Age\" Feature Regressor' + str(\n",
    "            lrf_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "\n",
    "        missing_age_test['Age_LRF'] = lrf_reg_grid.predict(missing_age_X_test)\n",
    "        print(missing_age_test['Age_LRF'][:4])\n",
    "\n",
    "        print('shape1',missing_age_test['Age'].shape,missing_age_test[['Age_GB','Age_LRF']].mode(axis=1).shape)\n",
    "        #missing_age_test['Age'] = missing_age_test[['Age_GB','Age_LRF']].mode(axis=1)\n",
    "        missing_age_test['Age'] = np.mean([missing_age_test['Age_GB'],missing_age_test['Age_LRF']])\n",
    "        print(missing_age_test['Age'][:4])\n",
    "        drop_col_not_req(missing_age_test, ['Age_GB', 'Age_LRF'])\n",
    "\n",
    "        return missing_age_test\n",
    "\n",
    "\n",
    "    # 筛选重要特征\n",
    "    def get_top_n_features(titanic_train_data_X, titanic_train_data_Y, top_n_features):\n",
    "        # 随机森林\n",
    "        rf_est = RandomForestClassifier(random_state=42)\n",
    "        rf_param_grid = {'n_estimators': [500], 'min_samples_split': [2, 3], 'max_depth': [20]}\n",
    "        rf_grid = model_selection.GridSearchCV(rf_est, rf_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        rf_grid.fit(titanic_train_data_X,titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best RF Params:' + str(rf_grid.best_params_))\n",
    "        print('Top N Features Best RF Score:' + str(rf_grid.best_score_))\n",
    "        print('Top N Features RF Train Error:' + str(rf_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_rf = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                              'importance': rf_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_rf = feature_imp_sorted_rf.head(top_n_features)['feature']\n",
    "        print('Sample 25 Features from RF Classifier')\n",
    "        print(str(features_top_n_rf[:25]))\n",
    "\n",
    "        # AdaBoost\n",
    "        ada_est = ensemble.AdaBoostClassifier(random_state=42)\n",
    "        ada_param_grid = {'n_estimators': [500], 'learning_rate': [0.5, 0.6]}\n",
    "        ada_grid = model_selection.GridSearchCV(ada_est, ada_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        ada_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best Ada Params:' + str(ada_grid.best_params_))\n",
    "        print('Top N Features Best Ada Score:' + str(ada_grid.best_score_))\n",
    "        print('Top N Features Ada Train Error:' + str(ada_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_ada = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                               'importance': ada_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_ada = feature_imp_sorted_ada.head(top_n_features)['feature']\n",
    "        print('Sample 25 Feature from Ada Classifier:')\n",
    "        print(str(features_top_n_ada[:25]))\n",
    "\n",
    "        # ExtraTree\n",
    "        et_est = ensemble.ExtraTreesClassifier(random_state=42)\n",
    "        et_param_grid = {'n_estimators': [500], 'min_samples_split': [3, 4], 'max_depth': [15]}\n",
    "        et_grid = model_selection.GridSearchCV(et_est, et_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "        et_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "        print('Top N Features Best ET Params:' + str(et_grid.best_params_))\n",
    "        print('Top N Features Best ET Score:' + str(et_grid.best_score_))\n",
    "        print('Top N Features ET Train Error:' + str(et_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "        feature_imp_sorted_et = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                              'importance': et_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "            'importance', ascending=False)\n",
    "        features_top_n_et = feature_imp_sorted_et.head(top_n_features)['feature']\n",
    "        print('Sample 25 Features from ET Classifier:')\n",
    "        print(str(features_top_n_et[:25]))\n",
    "\n",
    "        # 融合以上三个模型\n",
    "        features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et],\n",
    "                                   ignore_index=True).drop_duplicates()\n",
    "\n",
    "        return features_top_n\n",
    "\n",
    "\n",
    "    # 开始训练模型\n",
    "\n",
    "    train_data_org = pd.read_csv('../data/train.csv')\n",
    "    test_data_org = pd.read_csv('../data/test.csv')\n",
    "\n",
    "    # 将训练集和测试集合并\n",
    "\n",
    "    test_data_org['Survived'] = 0\n",
    "    combined_train_test = train_data_org.append(test_data_org)\n",
    "\n",
    "    # 特征工程\n",
    "\n",
    "    # 1. Embarked\n",
    "    # Embarkde中有两个NaN\n",
    "    if combined_train_test['Embarked'].isnull().sum() != 0:\n",
    "        combined_train_test['Embarked'].fillna(combined_train_test['Embarked'].mode().iloc[0], inplace=True)\n",
    "\n",
    "    emb_dummies_df = pd.get_dummies(combined_train_test['Embarked'],\n",
    "                                    prefix=combined_train_test[['Embarked']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, emb_dummies_df], axis=1)\n",
    "\n",
    "    # 2. Sex\n",
    "    sex_dummies_df = pd.get_dummies(combined_train_test['Sex'], prefix=combined_train_test[['Sex']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, sex_dummies_df], axis=1)\n",
    "\n",
    "    # 3. Name\n",
    "    # Title\n",
    "    combined_train_test['Title'] = combined_train_test['Name'].str.extract('.+,(.+)').str.extract(\n",
    "        '^(.+?)\\.').str.strip()\n",
    "    # print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "    title_Dict = {}\n",
    "    title_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\n",
    "    title_Dict.update(dict.fromkeys(['Jonkheer', 'Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\n",
    "    title_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\n",
    "    title_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\n",
    "    title_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\n",
    "    title_Dict.update(dict.fromkeys(['Master'], 'Master'))\n",
    "\n",
    "    combined_train_test['Title'] = combined_train_test['Title'].map(title_Dict)\n",
    "    # print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "    title_dummies_df = pd.get_dummies(combined_train_test['Title'], prefix=combined_train_test[['Title']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, title_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Name Length\n",
    "    combined_train_test['Name_Length'] = combined_train_test['Name'].str.len()\n",
    "    combined_train_test['Name_length_Category'] = combined_train_test['Name_Length'].map(name_length_category)\n",
    "    # print(combined_train_test['Name_length_Category'].groupby(by=combined_train_test['Name_length_Category']).count().sort_values(ascending=False))\n",
    "\n",
    "    le_name = LabelEncoder()\n",
    "    le_name.fit(np.array(['Very_Short_Name', 'Short_Name', 'Medium_Name', 'Long_Name']))\n",
    "    combined_train_test['Name_length_Category'] = le_name.transform(combined_train_test['Name_length_Category'])\n",
    "\n",
    "    print(combined_train_test[['Name_length_Category','Survived']].corr())\n",
    "    name_length_dummies_df = pd.get_dummies(combined_train_test['Name_length_Category'],\n",
    "                                            prefix=combined_train_test[['Name_length_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, name_length_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # First Name\n",
    "    combined_train_test['First_Name'] = combined_train_test['Name'].str.extract('^(.+?),').str.strip()\n",
    "    first_name_dummies_df = pd.get_dummies(combined_train_test['First_Name'],\n",
    "                                           prefix=combined_train_test[['First_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, first_name_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Last_Name\n",
    "    combined_train_test['Last_Name'] = combined_train_test['Name'].str.split('\\.').str[1].str.strip()\n",
    "    combined_train_test['Last_Name'] = combined_train_test['Last_Name'].str.strip('\\([^)]*\\)')\n",
    "    combined_train_test['Last_Name'].fillna(combined_train_test['Name'].str.split('\\.').str[1].str.strip())\n",
    "    # print(combined_train_test['Last_Name'].groupby(by = combined_train_test['Last_Name']).count().sort_values(ascending = False)[:5])\n",
    "    last_name_dummies_df = pd.get_dummies(combined_train_test['Last_Name'],\n",
    "                                          prefix=combined_train_test[['Last_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, last_name_dummies_df], axis=1)\n",
    "\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # Original_Name\n",
    "    combined_train_test['Original_Name'] = combined_train_test['Name'].str.split('\\((.*?)\\)').str[1].str.strip(\n",
    "        '\\\"').str.strip()\n",
    "    # print(combined_train_test['Original_Name'].groupby(by = combined_train_test['Original_Name']).count().sort_values(ascending = False)[:5])\n",
    "    original_name_dummies_df = pd.get_dummies(combined_train_test['Original_Name'],\n",
    "                                              prefix=combined_train_test[['Original_Name']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, original_name_dummies_df], axis=1)\n",
    "\n",
    "    # 4. Fare\n",
    "    # 填充NaN\n",
    "    if combined_train_test['Fare'].isnull().sum() != 0:\n",
    "        combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "            combined_train_test.groupby('Pclass').transform('mean'))\n",
    "\n",
    "    # 将多人船票的价格平均到每人\n",
    "    combined_train_test['Group_Ticket'] = combined_train_test['Fare'].groupby(\n",
    "        by=combined_train_test['Ticket']).transform('count')\n",
    "    combined_train_test['Fare'] = combined_train_test['Fare'] / combined_train_test['Group_Ticket']\n",
    "    combined_train_test.drop(['Group_Ticket'], axis=1, inplace=True)\n",
    "\n",
    "    # 去除Fare为0的项\n",
    "    if sum(n == 0 for n in combined_train_test.Fare.values.flatten()) > 0:\n",
    "        combined_train_test.loc[combined_train_test.Fare == 0, 'Fare'] = np.nan\n",
    "        combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "            combined_train_test.groupby('Pclass').transform('mean'))\n",
    "    # print(combined_train_test['Fare'].describe())\n",
    "    # 建立Fare Category\n",
    "    combined_train_test['Fare_Category'] = combined_train_test['Fare'].map(fare_category)\n",
    "    le_fare = LabelEncoder()\n",
    "    le_fare.fit(np.array(['Very_Low_Fare', 'Low_Fare', 'Med_Fare', 'High_Fare', 'Very_High_Fare']))\n",
    "    combined_train_test['Fare_Category'] = le_fare.transform(combined_train_test['Fare_Category'])\n",
    "\n",
    "    fare_cat_dummies_df = pd.get_dummies(combined_train_test['Fare_Category'],\n",
    "                                         prefix=combined_train_test[['Fare_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, fare_cat_dummies_df], axis=1)\n",
    "    # print(combined_train_test['Fare_Category'].groupby(by = combined_train_test['Fare_Category']).count().sort_values(ascending = False))\n",
    "\n",
    "    # 5. Pclass\n",
    "    # print(combined_train_test['Fare'].groupby(by = combined_train_test['Pclass']).mean())\n",
    "    Pclass_1_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([1]).values[0]\n",
    "    Pclass_2_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([2]).values[0]\n",
    "    Pclass_3_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([3]).values[0]\n",
    "    # 建立Pclass_Fare Category\n",
    "    combined_train_test['Pclass_Fare_Category'] = combined_train_test.apply(pclass_fare_category, args=(\n",
    "    Pclass_1_mean_fare, Pclass_2_mean_fare, Pclass_3_mean_fare), axis=1)\n",
    "    # print(combined_train_test['Pclass_Fare_Category'].groupby(by = combined_train_test['Pclass_Fare_Category']).count().sort_values(ascending = False))\n",
    "    p_fare = LabelEncoder()\n",
    "    p_fare.fit(np.array(\n",
    "        ['Pclass_1_Low_Fare', 'Pclass_1_High_Fare', 'Pclass_2_Low_Fare', 'Pclass_2_High_Fare', 'Pclass_3_Low_Fare',\n",
    "         'Pclass_3_High_Fare']))\n",
    "    combined_train_test['Pclass_Fare_Category'] = p_fare.transform(combined_train_test['Pclass_Fare_Category'])\n",
    "\n",
    "    # 6. Parch and SibSp\n",
    "\n",
    "    combined_train_test['Family_Size'] = combined_train_test['Parch'] + combined_train_test['SibSp'] + 1\n",
    "    combined_train_test['Family_Size_Category'] = combined_train_test['Family_Size'].map(family_size_category)\n",
    "\n",
    "    le_family = LabelEncoder()\n",
    "    le_family.fit(np.array(['Single', 'Small_Family', 'Large_Family']))\n",
    "    combined_train_test['Family_Size_Category'] = le_family.transform(combined_train_test['Family_Size_Category'])\n",
    "\n",
    "    fam_size_cat_dummies_df = pd.get_dummies(combined_train_test['Family_Size_Category'],\n",
    "                                             prefix=combined_train_test[['Family_Size_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test, fam_size_cat_dummies_df], axis=1)\n",
    "    # print(combined_train_test)\n",
    "\n",
    "    # 7. Age\n",
    "    # 填充Age中的NaN值\n",
    "\n",
    "    combined_train_test['Age_Null'] = combined_train_test['Age'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "\n",
    "    missing_age_df = pd.DataFrame(\n",
    "        combined_train_test[['Age', 'Parch', 'Sex', 'SibSp', 'Family_Size', 'Family_Size_Category',\n",
    "                             'Title', 'Fare', 'Fare_Category', 'Pclass', 'Embarked']])\n",
    "    missing_age_df = pd.get_dummies(missing_age_df,\n",
    "                                    columns=['Title', 'Family_Size_Category', 'Fare_Category', 'Sex', 'Pclass',\n",
    "                                             'Embarked'])\n",
    "    missing_age_train = missing_age_df[missing_age_df['Age'].notnull()]\n",
    "    missing_age_test = missing_age_df[missing_age_df['Age'].isnull()]\n",
    "    combined_train_test.loc[(combined_train_test.Age.isnull()), 'Age'] = fill_missing_age(missing_age_train,\n",
    "                                                                                          missing_age_test)\n",
    "    #print(combined_train_test.describe())\n",
    "\n",
    "    #检查是否有异常值\n",
    "    if sum(n<0 for n in combined_train_test.Age.values.flatten()) > 0:\n",
    "        combined_train_test.loc[combined_train_test.Age < 0,'Age'] = np.nan\n",
    "        combined_train_test['Age'] = combined_train_test[['Age']].fillna(combined_train_test.groupby('Title').transform('mean'))\n",
    "    #print(combined_train_test['Age'].groupby(by=combined_train_test['Title']).mean().sort_values(ascending=True))\n",
    "\n",
    "    # 建立Age_Category\n",
    "    combined_train_test['Age_Category'] = combined_train_test['Age'].map(age_group_category)\n",
    "    le_age = LabelEncoder()\n",
    "    le_age.fit(np.array(['Baby','Toddler','Child','Teenager','Adult','Middle_Aged','Senior_Citizen','Old']))\n",
    "    combined_train_test['Age_Category'] = le_age.transform(combined_train_test['Age_Category'])\n",
    "    age_cat_dummies_df = pd.get_dummies(combined_train_test['Age_Category'],\n",
    "                                        prefix=combined_train_test[['Age_Category']].columns[0])\n",
    "    combined_train_test = pd.concat([combined_train_test,age_cat_dummies_df],axis=1)\n",
    "\n",
    "    #8. Ticket\n",
    "    combined_train_test['Ticket_Letter'] = combined_train_test['Ticket'].str.split().str[0]\n",
    "    combined_train_test['Ticket_Letter'] = combined_train_test['Ticket_Letter'].apply(lambda x:np.nan if x.isnumeric() else x)\n",
    "    combined_train_test['Ticket_Number'] = combined_train_test['Ticket'].apply(lambda x: pd.to_numeric(x,errors='coerce'))\n",
    "    combined_train_test['Ticket_Number'].fillna(0,inplace=True)\n",
    "    combined_train_test = pd.get_dummies(combined_train_test,columns=['Ticket','Ticket_Letter'])\n",
    "    #print(combined_train_test.shape)\n",
    "\n",
    "    #9. Cabin\n",
    "    combined_train_test['Cabin_Letter'] = combined_train_test['Cabin'].apply(lambda x:str(x)[0] if pd.notnull(x) else x)\n",
    "    combined_train_test = pd.get_dummies(combined_train_test,columns=['Cabin','Cabin_Letter'])\n",
    "    #print(combined_train_test.shape)\n",
    "\n",
    "    #10. 将Age和Fare正则化\n",
    "    scale_age_fare = preprocessing.StandardScaler().fit(combined_train_test[['Age','Fare']])\n",
    "    combined_train_test[['Age','Fare']] = scale_age_fare.transform(combined_train_test[['Age','Fare']])\n",
    "\n",
    "    #11. 弃掉无用列\n",
    "    combined_train_test.drop(['Name', 'PassengerId', 'Embarked', 'Sex', 'Title', 'Fare_Category',\n",
    "                              'Family_Size_Category', 'Age_Category', 'First_Name', 'Last_Name',\n",
    "                              'Original_Name', 'Name_length_Category'],axis=1,inplace=True)\n",
    "    print(combined_train_test.describe())\n",
    "    #12. 整理数据\n",
    "\n",
    "    train_data = combined_train_test[:891]\n",
    "    test_data = combined_train_test[891:]\n",
    "\n",
    "    titanic_train_data_X = train_data.drop(['Survived'],axis=1)\n",
    "    titanic_train_data_Y = train_data['Survived']\n",
    "\n",
    "    titanic_test_data_X = test_data.drop(['Survived'],axis=1)\n",
    "\n",
    "    #13. 利用特征值重要性排名来去除无用列\n",
    "    feature_to_pick = 250\n",
    "    feature_top_n = get_top_n_features(titanic_train_data_X,titanic_train_data_Y,feature_to_pick)\n",
    "    print('Total Feature:'+str(combined_train_test.shape))\n",
    "    print('Picked Feature'+str(feature_top_n.shape))\n",
    "\n",
    "    titanic_train_data_X = titanic_train_data_X[feature_top_n]\n",
    "    del titanic_train_data_X['Ticket_Number']\n",
    "    titanic_test_data_X = titanic_test_data_X[feature_top_n]\n",
    "    del titanic_test_data_X['Ticket_Number']\n",
    "\n",
    "    #14.建立模型\n",
    "    rf_est = ensemble.RandomForestClassifier(n_estimators = 750, criterion = 'gini', max_features = 'sqrt',\n",
    "                                             max_depth = 3, min_samples_split = 4, min_samples_leaf = 2,\n",
    "                                             n_jobs = 50, random_state = 42, verbose = 1)\n",
    "    gbm_est = ensemble.GradientBoostingClassifier(n_estimators=900, learning_rate=0.0008, loss='exponential',\n",
    "                                                  min_samples_split=3, min_samples_leaf=2, max_features='sqrt',\n",
    "                                                  max_depth=3, random_state=42, verbose=1)\n",
    "    et_est = ensemble.ExtraTreesClassifier(n_estimators=750, max_features='sqrt', max_depth=35, n_jobs=50,\n",
    "                                           criterion='entropy', random_state=42, verbose=1)\n",
    "\n",
    "    voting_est = ensemble.VotingClassifier(estimators = [('rf', rf_est),('gbm', gbm_est),('et', et_est)],\n",
    "                                       voting = 'soft', weights = [3,5,2],\n",
    "                                       n_jobs = 50)\n",
    "    voting_est.fit(titanic_train_data_X,titanic_train_data_Y)\n",
    "    print('VotingClassifier Score:' + str(voting_est.score(titanic_train_data_X,titanic_train_data_Y)))\n",
    "    print('VotingClassifier Estimators:' + str(voting_est.estimators_))\n",
    "\n",
    "    #预测\n",
    "    titanic_test_data_X['Survived'] = voting_est.predict(titanic_test_data_X)\n",
    "\n",
    "    submission = pd.DataFrame({'PassengerId':test_data_org.loc[:,'PassengerId'],\n",
    "                               'Survived':titanic_test_data_X.loc[:,'Survived']})\n",
    "\n",
    "    submission.to_csv('submission_result.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
