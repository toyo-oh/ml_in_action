{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "['LOW', 'AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI', 'BWT']\n",
      "189\n",
      "189\n",
      "709\n",
      "189\n",
      "['28' '113' '1' '1' '1' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "birthdata_url = 'http://springer.bme.gatech.edu/Ch17.Logistic/Logisticdat/birthweight.dat'\n",
    "birth_file = requests.get(birthdata_url)\n",
    "birth_data = birth_file.text.split('\\r\\n')\n",
    "print(len(birth_data))\n",
    "\n",
    "birth_header = [x for x in birth_data[0].split('\\t')]\n",
    "print(birth_header)\n",
    "\n",
    "birth_data = [[ x for x in y.split('\\t')] for y in birth_data[1:-2]]\n",
    "print(len(birth_data))\n",
    "\n",
    "y_vals = np.array([x[8] for x in birth_data])\n",
    "print(len(y_vals))\n",
    "print(y_vals[0])\n",
    "\n",
    "#enumerate example\n",
    "# >>>seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "# >>> list(enumerate(seasons))\n",
    "# [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n",
    "cols = ['AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI']\n",
    "x_vals = np.array([[x[ix] for ix, feature in enumerate(birth_header) if feature in cols ] for x in birth_data])\n",
    "print(len(x_vals))\n",
    "print(x_vals[0])\n",
    "\n",
    "# 列表推导式\n",
    "# 列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。\n",
    "# 它的结构是在一个中括号里包含一个表达式，然后是一个for语句，然后是 0 个或多个 for 或者 if 语句。\n",
    "# 那个表达式可以是任意的，意思是你可以在列表中放入任意类型的对象。\n",
    "# 返回结果将是一个新的列表，在这个以 if 和 for 语句为上下文的表达式运行完成之后产生。\n",
    "#  列表推导式的执行顺序：各语句之间是嵌套关系，左边第二个语句是最外层，依次往右进一层，左边第一条语句是最后一层。\n",
    "\n",
    "# [x*y for x in range(1,5) if x > 2 for y in range(1,4) if y < 3]\n",
    "# 他的执行顺序是:\n",
    "\n",
    "# for x in range(1,5)\n",
    "#     if x > 2\n",
    "#         for y in range(1,4)\n",
    "#             if y < 3\n",
    "#                 x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  709.,  1021.,  1135.,  1330.,  1474.,  1588.,  1588.,  1701.,\n",
       "        1729.,  1790.,  1818.,  1885.,  1893.,  1899.,  1928.,  1928.,\n",
       "        1928.,  1936.,  1970.,  2055.,  2055.,  2082.,  2084.,  2084.,\n",
       "        2100.,  2125.,  2126.,  2187.,  2187.,  2211.,  2225.,  2240.,\n",
       "        2240.,  2282.,  2296.,  2296.,  2301.,  2325.,  2353.,  2353.,\n",
       "        2367.,  2381.,  2381.,  2381.,  2395.,  2410.,  2410.,  2414.,\n",
       "        2424.,  2438.,  2442.,  2450.,  2466.,  2466.,  2466.,  2495.,\n",
       "        2495.,  2495.,  2495.,  2523.,  2551.,  2557.,  2594.,  2600.,\n",
       "        2622.,  2637.,  2637.,  2663.,  2665.,  2722.,  2733.,  2750.,\n",
       "        2750.,  2769.,  2769.,  2778.,  2782.,  2807.,  2821.,  2835.,\n",
       "        2835.,  2836.,  2863.,  2877.,  2877.,  2906.,  2920.,  2920.,\n",
       "        2920.,  2920.,  2948.,  2948.,  2977.,  2977.,  2977.,  2977.,\n",
       "        2992.,  3005.,  3033.,  3042.,  3062.,  3062.,  3062.,  3076.,\n",
       "        3076.,  3080.,  3090.,  3090.,  3090.,  3100.,  3104.,  3132.,\n",
       "        3147.,  3175.,  3175.,  3203.,  3203.,  3203.,  3225.,  3225.,\n",
       "        3232.,  3232.,  3234.,  3260.,  3274.,  3274.,  3303.,  3317.,\n",
       "        3317.,  3317.,  3321.,  3331.,  3374.,  3374.,  3402.,  3416.,\n",
       "        3430.,  3444.,  3459.,  3460.,  3473.,  3475.,  3487.,  3544.,\n",
       "        3572.,  3572.,  3586.,  3600.,  3614.,  3614.,  3629.,  3629.,\n",
       "        3637.,  3643.,  3651.,  3651.,  3651.,  3651.,  3699.,  3728.,\n",
       "        3756.,  3770.,  3770.,  3770.,  3790.,  3799.,  3827.,  3856.,\n",
       "        3860.,  3860.,  3884.,  3884.,  3912.,  3940.,  3941.,  3941.,\n",
       "        3969.,  3983.,  3997.,  3997.,  4054.,  4054.,  4111.,  4153.,\n",
       "        4167.,  4174.,  4238.,  4593.,  4990.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert typeA to typeB\n",
    "#numpy.ndarray.astype\n",
    "#Copy of the array, cast to a specified type.\n",
    "\n",
    "# >>> x = np.array([1, 2, 2.5])\n",
    "# >>> x.astype(int)\n",
    "# array([1, 2, 2])\n",
    "\n",
    "x_vals.astype(np.float32)\n",
    "y_vals.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =3 \n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a uniform random sample from np.arange(5) of size 3:\n",
    "# >>> np.random.choice(5, 3)\n",
    "# array([0, 3, 4])\n",
    "# >>> #This is equivalent to np.random.randint(0,5,3)\n",
    "\n",
    "# Generate a uniform random sample from np.arange(5) of size 3 without replacement:\n",
    "# >>> np.random.choice(5, 3, replace=False)\n",
    "# array([3,1,0])\n",
    "# >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n",
    "\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace = False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) -  set(train_indices)))\n",
    "\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"normfor4.png\" width=\"400\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#关于归一化，标准化由于英文翻译造成的混乱，总结如下：\n",
    "\n",
    "# Rescaling →　「归一化：使数据分布在0～1之间，从椭圆变为圆形，便于梯度下降」（下面图片中：第一种）\n",
    "#Standardization →　「标准化：使数据处理后符合标准正态分布」（下面图片中：第三种）\n",
    "#Regularization →　 「正则化」（如L1，L2，不用在这里所说的特征处理上）\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"normfor4.png\", width=400, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype <U3 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#sklearn  https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-normalization\n",
    "# 标准化操作  StandardScaler() （6.3.1. Standardization, or mean removal and variance scaling）  \n",
    "#归一化操作   MinMaxScaler() （6.3.1.1. Scaling features to a range） →　归一化是特殊的标准化，相当于 （均值=0，方差=1） 的情况\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "x_vals_train = min_max_scaler.fit_transform(x_vals_train)\n",
    "x_vals_test  = min_max_scaler.fit_transform(x_vals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_cols(m):\n",
    "#     col_max = m.max(axis=0)\n",
    "#     col_min = m.min(axis=0)\n",
    "#     return (m - col_min)/ (col_max - col_min)\n",
    "\n",
    "# x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n",
    "# x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.random_normal(shape,mean=0.0,stddev=1.0,dtype=tf.float32,seed=None,name=None)\n",
    "# shape：一个一维整数张量或Python数组。代表张量的形状。\n",
    "# mean：数据类型为dtype的张量值或Python值。是正态分布的均值。\n",
    "# stddev：数据类型为dtype的张量值或Python值。是正态分布的标准差。\n",
    "# dtype： 输出的数据类型。\n",
    "# seed：一个Python整数。是随机种子。\n",
    "# name： 操作的名称(可选)\n",
    "\n",
    "#initialize w and b\n",
    "def init_weight(shape, st_dev):\n",
    "    weight = tf.Variable(tf.random_normal(shape, stddev= st_dev))\n",
    "    return weight\n",
    "\n",
    "def init_bias(shape, st_dev):\n",
    "    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize placeholder\n",
    "x_data = tf.placeholder(shape=[None,7], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None,1],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full connected\n",
    "def fully_connected(input_layer, weights, biases):\n",
    "    layer = tf.add(tf.matmul(input_layer, weights), biases)\n",
    "    return tf.nn.relu(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三个隐藏层的神经元个数分别为25，10，3\n",
    "# 第一隐藏层之间的变量：25*7（w1）+25(b1)=200\n",
    "# 第二隐藏层之间的变量：25*10（w2）+10（b2）=260\n",
    "# 第三隐藏层之间的变量：10*3（w3）+3（b3）=33\n",
    "# 输出层的变量：3*1（w4）+1(b4)=4\n",
    "# 需要拟合的变量总共为：200+260+33+4=497\n",
    "\n",
    "#Create first layer(25 hidden nodes)\n",
    "weight1 = init_weight(shape=[7,25], st_dev=10.0)\n",
    "bias1 = init_bias(shape=[25], st_dev=10.0)\n",
    "layer1 = fully_connected(x_data, weight1, bias1)\n",
    "\n",
    "#Create second layer(10 hidden nodes)\n",
    "weight2 = init_weight(shape=[25,10], st_dev=10.0)\n",
    "bias2 = init_bias(shape=[10], st_dev=10.0)\n",
    "layer2 = fully_connected(layer1, weight2, bias2)\n",
    "\n",
    "#Create third layer(3 hidden nodes)\n",
    "weight3 = init_weight(shape=[10,3], st_dev=10.0)\n",
    "bias3 = init_bias(shape=[3], st_dev=10.0)\n",
    "layer3 = fully_connected(layer2, weight3, bias3)\n",
    "\n",
    "#Create output layer(1 output values)\n",
    "weight4 = init_weight(shape=[3,1], st_dev=10.0)\n",
    "bias4 = init_bias(shape=[1], st_dev=10.0)\n",
    "final_output = fully_connected(layer3, weight4, bias4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss(L1) , Optimizer(AdamOptimizer)\n",
    "loss = tf.reduce_mean(tf.abs(y_target-final_output))\n",
    "my_opt = tf.train.AdamOptimizer(0.05)\n",
    "train_step= my_opt.minimize(loss)\n",
    "init =  tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 25 Loss =398.422\n",
      "Generation: 50 Loss =455.814\n",
      "Generation: 75 Loss =624.154\n",
      "Generation: 100 Loss =505.198\n",
      "Generation: 125 Loss =501.907\n",
      "Generation: 150 Loss =449.634\n",
      "Generation: 175 Loss =366.31\n",
      "Generation: 200 Loss =454.773\n",
      "Generation: 225 Loss =550.58\n",
      "Generation: 250 Loss =511.725\n",
      "Generation: 275 Loss =425.742\n",
      "Generation: 300 Loss =341.477\n",
      "Generation: 325 Loss =347.954\n",
      "Generation: 350 Loss =314.968\n",
      "Generation: 375 Loss =460.222\n",
      "Generation: 400 Loss =480.144\n",
      "Generation: 425 Loss =442.029\n",
      "Generation: 450 Loss =448.594\n",
      "Generation: 475 Loss =573.863\n",
      "Generation: 500 Loss =292.359\n"
     ]
    }
   ],
   "source": [
    "#Initialize the loss vectors\n",
    "loss_vec = []\n",
    "test_loss = []\n",
    "batch_size = 25\n",
    "\n",
    "for i in range(500):\n",
    "    # Choose random indices for batch selection\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    # Get random batch\n",
    "    rand_x = x_vals_train[rand_index]\n",
    "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "    # Run the training step\n",
    "    sess.run(train_step,feed_dict={x_data:rand_x, y_target:rand_y})\n",
    "    # Get and store the train loss\n",
    "    train_temp_loss = sess.run(loss,feed_dict={x_data:rand_x, y_target:rand_y})\n",
    "    loss_vec.append(train_temp_loss)\n",
    "    # Get and store the test loss\n",
    "    test_temp_loss = sess.run(loss,feed_dict={x_data:x_vals_test, y_target:np.transpose([y_vals_test])})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    \n",
    "    if(i+1)%25==0:\n",
    "        print('Generation: '+str(i+1)+' Loss ='+str(train_temp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actuals = np.array([x[1] for x in birth_data]) \n",
    "# test_actuals = actuals[test_indices] \n",
    "# train_actuals = actuals[train_indices] \n",
    "# test_preds = [x[0] for x in sess.run(final_output, feed_dict={x_data: x_vals_test})] \n",
    "# train_preds = [x[0] for x in sess.run(final_output, feed_dict={x_data: x_vals_train})] \n",
    "# test_preds = np.array([1.0 if x<2500.0 else 0.0 for x in test_preds]) \n",
    "# train_preds = np.array([1.0 if x<2500.0 else 0.0 for x in train_preds]) \n",
    "# # Print out accuracies \n",
    "# test_acc = np.mean([x==y for x,y in zip(test_preds, test_actuals)]) \n",
    "# train_acc = np.mean([x==y for x,y in zip(train_preds, train_actuals)]) \n",
    "# print('On predicting the category of low birthweight from regression output (<2500g):') \n",
    "# print('Test Accuracy: {}'.format(test_acc)) \n",
    "# print('Train Accuracy: {}'.format(train_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
