{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting temp/train-images-idx3-ubyte.gz\n",
      "Extracting temp/train-labels-idx1-ubyte.gz\n",
      "Extracting temp/t10k-images-idx3-ubyte.gz\n",
      "Extracting temp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_dir = 'temp'\n",
    "mnist = read_data_sets(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# train data & test data(28*28 =784)\n",
    "\n",
    "# np.reshape: Gives a new shape to an array without changing its data.\n",
    "# Parameters:\n",
    "# a : array_like  Array to be reshaped.\n",
    "# newshape : int or tuple of ints\n",
    "# order : {‘C’, ‘F’, ‘A’}, optional\n",
    "# Returns:reshaped_array : ndarray\n",
    "\n",
    "train_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.test.images])\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "print( train_xdata[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.conv2d(卷积层) ###\n",
    "**Computes a 2-D convolution given 4-D input and filters tensors.**  \n",
    "**tf.nn.conv2d(input, filters, strides, padding, data_format='NHWC', dilations=None, name=None)**  \n",
    "**input**: A 4-D tensor. [batch, in_height, in_width, in_channels]  \n",
    "**filters**: A 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]  \n",
    "**strides**: Must have strides[0] = strides[3] = 1.   \n",
    "For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].  \n",
    "**padding**: \"SAME\" or \"VALID\"   \n",
    "**data_format**: An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".   \n",
    "\n",
    "VALID，输出的形状为：$ new\\_height = new\\_width =\\lceil  \\frac{(W+2P–F)}{S} \\rceil+1  $   \n",
    "SAME，输出的形状为：$ new\\_height=new\\_width=\\lceil \\frac{ W } { S}\\rceil $   \n",
    "\n",
    "W为输入的size，P为Padding的宽度,F为filter的size，S为步长，⌈⌉为向上取整符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.max_pool(池化层) ###\n",
    "**Performs the max pooling on the input.**  \n",
    "**tf.nn.max_pool(input, ksize, strides, padding, data_format=None, name=None)**  \n",
    "**input**: Tensor of rank N+2, of shape [batch_size] + input_spatial_shape + [num_channels] if data_format does not start with \"NC\" (default), or [batch_size, num_channels] + input_spatial_shape if data_format starts with \"NC\". Pooling happens over the spatial dimensions only.  \n",
    "**ksize**: An int or list of ints that has length 1, N or N+2. The size of the window for each dimension of the input tensor.  \n",
    "**strides**: An int or list of ints that has length 1, N or N+2. The stride of the sliding window for each dimension of the input tensor.  \n",
    "**padding**: A string, either 'VALID' or 'SAME'. The padding algorithm. See the \"returns\" section of tf.nn.convolution for details.  \n",
    "**data_format**: A string. Specifies the channel dimension. For N=1 it can be either \"NWC\" (default) or \"NCW\", for N=2 it can be either \"NHWC\" (default) or \"NCHW\" and for N=3 either \"NDHWC\" (default) or \"NCDHW\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于图像是灰度图，所以该图像的深度为1，颜色通道数为1\n",
    "\n",
    "batch_size = 100 #训练集一次批量的大小\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500 # 测试集大小\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "#\n",
    "target_size = max(train_labels)+1\n",
    "num_channels = 1\n",
    "#迭代次数\n",
    "generations = 500\n",
    "\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "fully_connected_size1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training input\n",
    "x_input_shape = (batch_size, image_height, image_width, num_channels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "#testing input\n",
    "eval_input_shape = (evaluation_size, image_height, image_width, num_channels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.truncated_normal : Outputs random values from a truncated normal distribution.\n",
    "# tf.random.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "\n",
    "#初始化 卷积核w 和 偏置项b \n",
    "conv1_w = tf.Variable(tf.truncated_normal(shape=[4,4,num_channels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "conv1_b = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_w = tf.Variable(tf.truncated_normal(shape=[4,4,conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "conv2_b = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_width = image_width//(max_pool_size1*max_pool_size2)\n",
    "resulting_height = image_height//(max_pool_size1*max_pool_size2)\n",
    "\n",
    "full_input_size = resulting_width * resulting_height * conv2_features\n",
    "full1_w = tf.Variable(tf.truncated_normal(shape=[full_input_size,fully_connected_size1], stddev=0.1, dtype= tf.float32))\n",
    "full1_b = tf.Variable(tf.truncated_normal(shape=[fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "full2_w = tf.Variable(tf.truncated_normal(shape=[fully_connected_size1,target_size], stddev=0.1, dtype= tf.float32))\n",
    "full2_b = tf.Variable(tf.truncated_normal(shape=[target_size], stddev=0.1, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv_net(input_data):\n",
    "    #First Conv-ReLU-MaxPool Layer\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_w, strides=[1,1,1,1], padding='SAME')\n",
    "    print(\"conv1 shape:\"+str(conv1.get_shape()))\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b))\n",
    "    print(\"relu1 shape:\"+str(relu1.get_shape()))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1], strides=[1, max_pool_size1,max_pool_size1, 1], padding='SAME')\n",
    "    print(\"max_pool1 shape:\"+str(max_pool1.get_shape()))\n",
    "    \n",
    "    #Second Conv-ReLu-MaxPoolLayer \n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_w, strides=[1,1,1,1], padding='SAME')\n",
    "    print(\"conv2 shape:\"+str(conv2.get_shape()))\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "    print(\"relu2 shape:\"+str(relu2.get_shape()))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1], strides=[1, max_pool_size2,max_pool_size2, 1], padding='SAME')\n",
    "    print(\"max_pool2 shape:\"+str(max_pool2.get_shape()))\n",
    "    \n",
    "    # fully connected layer\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1]*final_conv_shape[2]*final_conv_shape[3]\n",
    "    flat_ouput = tf.reshape(max_pool2, [final_conv_shape[0],final_shape])\n",
    "    print(\"flat_ouput:\"+str(flat_ouput.get_shape()))\n",
    "    \n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_ouput, full1_w), full1_b))\n",
    "    print(\"fully_connected1:\"+str(fully_connected1.get_shape()))\n",
    "    final_model_output = tf.nn.relu(tf.add(tf.matmul(fully_connected1, full2_w), full2_b))\n",
    "    print(\"final_model_output:\"+str(final_model_output.get_shape()))\n",
    "    return (final_model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape:(100, 28, 28, 25)\n",
      "relu1 shape:(100, 28, 28, 25)\n",
      "max_pool1 shape:(100, 14, 14, 25)\n",
      "conv2 shape:(100, 14, 14, 50)\n",
      "relu2 shape:(100, 14, 14, 50)\n",
      "max_pool2 shape:(100, 7, 7, 50)\n",
      "flat_ouput:(100, 2450)\n",
      "fully_connected1:(100, 100)\n",
      "final_model_output:(100, 10)\n"
     ]
    }
   ],
   "source": [
    "model_output = my_conv_net(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape:(500, 28, 28, 25)\n",
      "relu1 shape:(500, 28, 28, 25)\n",
      "max_pool1 shape:(500, 14, 14, 25)\n",
      "conv2 shape:(500, 14, 14, 50)\n",
      "relu2 shape:(500, 14, 14, 50)\n",
      "max_pool2 shape:(500, 7, 7, 50)\n",
      "flat_ouput:(500, 2450)\n",
      "fully_connected1:(500, 100)\n",
      "final_model_output:(500, 10)\n"
     ]
    }
   ],
   "source": [
    "test_model_output = my_conv_net(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "# Computes sparse softmax cross entropy between logits and labels.\n",
    "\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#     labels, logits, name=None\n",
    "# )\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_target, logits=model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step = my_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(model_output)\n",
    "test_prediction = tf.nn.softmax(test_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, targets):\n",
    "    batch_predictons = np.argmax(logits, axis=1)\n",
    "    num_correct = np.sum(np.equal(batch_predictons, targets))\n",
    "    return (100. * num_correct/batch_predictons.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 5. Train Loss: 2.32. Train Acc (Test Acc): 14.00 (18.40)\n",
      "Generation # 10. Train Loss: 2.23. Train Acc (Test Acc): 27.00 (25.80)\n",
      "Generation # 15. Train Loss: 2.18. Train Acc (Test Acc): 22.00 (21.20)\n",
      "Generation # 20. Train Loss: 2.06. Train Acc (Test Acc): 36.00 (28.00)\n",
      "Generation # 25. Train Loss: 2.10. Train Acc (Test Acc): 30.00 (31.80)\n",
      "Generation # 30. Train Loss: 1.98. Train Acc (Test Acc): 38.00 (36.20)\n",
      "Generation # 35. Train Loss: 2.09. Train Acc (Test Acc): 31.00 (37.20)\n",
      "Generation # 40. Train Loss: 1.82. Train Acc (Test Acc): 47.00 (46.60)\n",
      "Generation # 45. Train Loss: 1.79. Train Acc (Test Acc): 39.00 (43.60)\n",
      "Generation # 50. Train Loss: 1.56. Train Acc (Test Acc): 48.00 (42.20)\n",
      "Generation # 55. Train Loss: 1.80. Train Acc (Test Acc): 35.00 (49.60)\n",
      "Generation # 60. Train Loss: 1.65. Train Acc (Test Acc): 48.00 (44.20)\n",
      "Generation # 65. Train Loss: 1.65. Train Acc (Test Acc): 38.00 (43.40)\n",
      "Generation # 70. Train Loss: 1.63. Train Acc (Test Acc): 47.00 (45.80)\n",
      "Generation # 75. Train Loss: 1.51. Train Acc (Test Acc): 47.00 (51.20)\n",
      "Generation # 80. Train Loss: 1.59. Train Acc (Test Acc): 45.00 (51.60)\n",
      "Generation # 85. Train Loss: 1.45. Train Acc (Test Acc): 56.00 (48.80)\n",
      "Generation # 90. Train Loss: 1.33. Train Acc (Test Acc): 50.00 (52.00)\n",
      "Generation # 95. Train Loss: 1.33. Train Acc (Test Acc): 53.00 (52.60)\n",
      "Generation # 100. Train Loss: 1.39. Train Acc (Test Acc): 53.00 (55.20)\n",
      "Generation # 105. Train Loss: 1.39. Train Acc (Test Acc): 54.00 (54.00)\n",
      "Generation # 110. Train Loss: 1.41. Train Acc (Test Acc): 50.00 (54.20)\n",
      "Generation # 115. Train Loss: 1.31. Train Acc (Test Acc): 54.00 (49.40)\n",
      "Generation # 120. Train Loss: 1.56. Train Acc (Test Acc): 42.00 (54.00)\n",
      "Generation # 125. Train Loss: 1.25. Train Acc (Test Acc): 66.00 (55.20)\n",
      "Generation # 130. Train Loss: 1.34. Train Acc (Test Acc): 53.00 (57.80)\n",
      "Generation # 135. Train Loss: 1.47. Train Acc (Test Acc): 53.00 (56.60)\n",
      "Generation # 140. Train Loss: 1.33. Train Acc (Test Acc): 57.00 (53.80)\n",
      "Generation # 145. Train Loss: 1.43. Train Acc (Test Acc): 49.00 (52.80)\n",
      "Generation # 150. Train Loss: 1.21. Train Acc (Test Acc): 57.00 (56.80)\n",
      "Generation # 155. Train Loss: 1.16. Train Acc (Test Acc): 62.00 (57.60)\n",
      "Generation # 160. Train Loss: 1.28. Train Acc (Test Acc): 54.00 (52.80)\n",
      "Generation # 165. Train Loss: 1.14. Train Acc (Test Acc): 61.00 (60.60)\n",
      "Generation # 170. Train Loss: 1.15. Train Acc (Test Acc): 60.00 (56.60)\n",
      "Generation # 175. Train Loss: 1.24. Train Acc (Test Acc): 56.00 (62.00)\n",
      "Generation # 180. Train Loss: 1.25. Train Acc (Test Acc): 58.00 (55.00)\n",
      "Generation # 185. Train Loss: 1.34. Train Acc (Test Acc): 60.00 (61.00)\n",
      "Generation # 190. Train Loss: 1.07. Train Acc (Test Acc): 59.00 (58.00)\n",
      "Generation # 195. Train Loss: 1.13. Train Acc (Test Acc): 58.00 (59.60)\n",
      "Generation # 200. Train Loss: 1.17. Train Acc (Test Acc): 59.00 (61.20)\n",
      "Generation # 205. Train Loss: 0.98. Train Acc (Test Acc): 66.00 (62.60)\n",
      "Generation # 210. Train Loss: 1.00. Train Acc (Test Acc): 68.00 (57.60)\n",
      "Generation # 215. Train Loss: 1.05. Train Acc (Test Acc): 63.00 (61.80)\n",
      "Generation # 220. Train Loss: 1.27. Train Acc (Test Acc): 57.00 (66.40)\n",
      "Generation # 225. Train Loss: 0.97. Train Acc (Test Acc): 72.00 (64.00)\n",
      "Generation # 230. Train Loss: 1.19. Train Acc (Test Acc): 58.00 (62.00)\n",
      "Generation # 235. Train Loss: 0.99. Train Acc (Test Acc): 63.00 (62.00)\n",
      "Generation # 240. Train Loss: 1.09. Train Acc (Test Acc): 69.00 (63.20)\n",
      "Generation # 245. Train Loss: 1.09. Train Acc (Test Acc): 66.00 (56.20)\n",
      "Generation # 250. Train Loss: 1.20. Train Acc (Test Acc): 57.00 (62.20)\n",
      "Generation # 255. Train Loss: 1.21. Train Acc (Test Acc): 64.00 (60.20)\n",
      "Generation # 260. Train Loss: 0.99. Train Acc (Test Acc): 63.00 (58.80)\n",
      "Generation # 265. Train Loss: 1.16. Train Acc (Test Acc): 66.00 (63.00)\n",
      "Generation # 270. Train Loss: 1.05. Train Acc (Test Acc): 65.00 (59.60)\n",
      "Generation # 275. Train Loss: 1.14. Train Acc (Test Acc): 67.00 (61.00)\n",
      "Generation # 280. Train Loss: 1.10. Train Acc (Test Acc): 64.00 (64.60)\n",
      "Generation # 285. Train Loss: 1.10. Train Acc (Test Acc): 61.00 (66.60)\n",
      "Generation # 290. Train Loss: 0.96. Train Acc (Test Acc): 67.00 (63.00)\n",
      "Generation # 295. Train Loss: 1.20. Train Acc (Test Acc): 58.00 (57.00)\n",
      "Generation # 300. Train Loss: 1.35. Train Acc (Test Acc): 54.00 (61.20)\n",
      "Generation # 305. Train Loss: 1.15. Train Acc (Test Acc): 66.00 (64.20)\n",
      "Generation # 310. Train Loss: 1.10. Train Acc (Test Acc): 62.00 (64.80)\n",
      "Generation # 315. Train Loss: 0.80. Train Acc (Test Acc): 71.00 (66.80)\n",
      "Generation # 320. Train Loss: 1.06. Train Acc (Test Acc): 67.00 (64.80)\n",
      "Generation # 325. Train Loss: 0.91. Train Acc (Test Acc): 75.00 (69.60)\n",
      "Generation # 330. Train Loss: 0.84. Train Acc (Test Acc): 74.00 (75.20)\n",
      "Generation # 335. Train Loss: 1.00. Train Acc (Test Acc): 68.00 (73.20)\n",
      "Generation # 340. Train Loss: 0.79. Train Acc (Test Acc): 77.00 (74.00)\n",
      "Generation # 345. Train Loss: 0.98. Train Acc (Test Acc): 68.00 (76.40)\n",
      "Generation # 350. Train Loss: 1.01. Train Acc (Test Acc): 72.00 (74.80)\n",
      "Generation # 355. Train Loss: 0.89. Train Acc (Test Acc): 71.00 (72.80)\n",
      "Generation # 360. Train Loss: 0.93. Train Acc (Test Acc): 80.00 (71.80)\n",
      "Generation # 365. Train Loss: 0.95. Train Acc (Test Acc): 68.00 (74.00)\n",
      "Generation # 370. Train Loss: 0.84. Train Acc (Test Acc): 72.00 (77.60)\n",
      "Generation # 375. Train Loss: 0.96. Train Acc (Test Acc): 71.00 (71.80)\n",
      "Generation # 380. Train Loss: 0.88. Train Acc (Test Acc): 70.00 (72.00)\n",
      "Generation # 385. Train Loss: 0.92. Train Acc (Test Acc): 75.00 (73.20)\n",
      "Generation # 390. Train Loss: 1.05. Train Acc (Test Acc): 62.00 (73.80)\n",
      "Generation # 395. Train Loss: 0.78. Train Acc (Test Acc): 77.00 (72.20)\n",
      "Generation # 400. Train Loss: 0.78. Train Acc (Test Acc): 75.00 (74.20)\n",
      "Generation # 405. Train Loss: 0.79. Train Acc (Test Acc): 75.00 (74.20)\n",
      "Generation # 410. Train Loss: 0.91. Train Acc (Test Acc): 70.00 (72.00)\n",
      "Generation # 415. Train Loss: 0.87. Train Acc (Test Acc): 73.00 (74.20)\n",
      "Generation # 420. Train Loss: 0.82. Train Acc (Test Acc): 77.00 (74.60)\n",
      "Generation # 425. Train Loss: 0.83. Train Acc (Test Acc): 74.00 (77.40)\n",
      "Generation # 430. Train Loss: 0.96. Train Acc (Test Acc): 70.00 (73.40)\n",
      "Generation # 435. Train Loss: 0.89. Train Acc (Test Acc): 78.00 (76.00)\n",
      "Generation # 440. Train Loss: 0.77. Train Acc (Test Acc): 77.00 (74.60)\n",
      "Generation # 445. Train Loss: 0.91. Train Acc (Test Acc): 74.00 (72.80)\n",
      "Generation # 450. Train Loss: 0.80. Train Acc (Test Acc): 78.00 (75.80)\n",
      "Generation # 455. Train Loss: 0.85. Train Acc (Test Acc): 73.00 (73.40)\n",
      "Generation # 460. Train Loss: 1.15. Train Acc (Test Acc): 64.00 (75.80)\n",
      "Generation # 465. Train Loss: 0.74. Train Acc (Test Acc): 78.00 (75.20)\n",
      "Generation # 470. Train Loss: 0.90. Train Acc (Test Acc): 68.00 (73.80)\n",
      "Generation # 475. Train Loss: 0.82. Train Acc (Test Acc): 73.00 (78.00)\n",
      "Generation # 480. Train Loss: 0.89. Train Acc (Test Acc): 74.00 (73.20)\n",
      "Generation # 485. Train Loss: 0.94. Train Acc (Test Acc): 74.00 (75.80)\n",
      "Generation # 490. Train Loss: 1.05. Train Acc (Test Acc): 72.00 (74.40)\n",
      "Generation # 495. Train Loss: 0.93. Train Acc (Test Acc): 75.00 (74.40)\n",
      "Generation # 500. Train Loss: 0.94. Train Acc (Test Acc): 72.00 (74.20)\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc =[]\n",
    "test_acc=[]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_xdata), size=batch_size)\n",
    "    rand_x = train_xdata[rand_index]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_index]\n",
    "    train_dict = {x_input: rand_x, y_target:rand_y}\n",
    "    sess.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    if(i+1)%5 == 0:\n",
    "        eval_index = np.random.choice(len(test_xdata), size= evaluation_size)\n",
    "        eval_x = test_xdata[eval_index]\n",
    "        eval_x = np.expand_dims(eval_x,3)\n",
    "        eval_y = test_labels[eval_index]\n",
    "        test_dict = {eval_input:eval_x, eval_target:eval_y}\n",
    "        test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc =get_accuracy(test_preds, eval_y)\n",
    "        \n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        \n",
    "        acc_and_loss =[(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the 6 of the last batch results: \n",
    "actuals = rand_y[0:6] \n",
    "predictions = np.argmax(temp_train_preds,axis=1)[0:6] \n",
    "images = np.squeeze(rand_x[0:6]) \n",
    "Nrows = 2 \n",
    "Ncols = 3 \n",
    "for i in range(6): \n",
    "    plt.subplot(Nrows, Ncols, i+1) \n",
    "    plt.imshow(np.reshape(images[i], [28,28]), cmap='Greys_r') \n",
    "    plt.title('Actual: ' + str(actuals[i]) + ' Pred: ' + str(predictions[i]), fontsize=10) \n",
    "    frame = plt.gca() \n",
    "    frame.axes.get_xaxis().set_visible(False) \n",
    "    frame.axes.get_yaxis().set_visible(False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
