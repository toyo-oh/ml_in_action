{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "\n",
    "data_dir = 'CIFAR10/'\n",
    "extract_folder = 'cifar-10-batches-bin'\n",
    "# root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "lr_decay = 0.9\n",
    "num_gens_to_wait = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vec_length = image_height * image_width * num_channels\n",
    "record_length = 1+ image_vec_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用队列读取文件的方式\n",
    "\n",
    "# 1. tf.train.string_input_producer 建立文件队列\n",
    "#   filename_queue = tf.train.string_input_producer(filenames, num_epochs, shuffle)\n",
    "\n",
    "#  2. tf.reader读取数据\n",
    "#   tf.WholeFileReader() 直接读取整个文件\n",
    "#   tf.FixedLengthRecordReader 二进制部分读取文件\n",
    "#   reader = tf.FixedLengthRecordReader(record_bytes = result._record_bytes)\n",
    "#   result.key, value = reader.read(filename_queue)\n",
    "\n",
    "# 3. tf.train.start_queue_runners启动队列\n",
    "\n",
    "def read_cifar_files(filename_queue, distort_image=True):\n",
    "    \n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    # tf.dtypes.DType（tf.uint8→8-bit unsigned integer.）\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    print(record_bytes)\n",
    "    \n",
    "    #extract label\n",
    "    #tf.slice: https://www.jianshu.com/p/71e6ef6c121b\n",
    "    image_label = tf.cast(tf.slice(record_bytes,[0],[1]), tf.int32)\n",
    "    print(image_label)\n",
    "    \n",
    "    #extract image\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes,[1],[image_vec_length]), [num_channels, image_height, image_width])\n",
    "    print(image_extracted)\n",
    "    \n",
    "    #reshape image\n",
    "    image_unit8image = tf.transpose(image_extracted, [1,2,0])\n",
    "    print(image_unit8image)\n",
    "    reshaped_image = tf.cast(image_unit8image, tf.float32)\n",
    "    print(reshaped_image)\n",
    "    \n",
    "    #randomly crop image\n",
    "    # tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "    #Resizes an image to a target width and height by either centrally cropping the image or padding it evenly with zeros.\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    if distort_image:\n",
    "        # tf.image.random_flip_left_right(image, seed=None)\n",
    "        # Randomly flip an image horizontally (left to right)\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        # tf.image.random_brightness(image, max_delta, seed=None)\n",
    "        #Adjust the brightness of images by a random factor.\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        # Adjust the contrast of an image or images by a random factor.\n",
    "        # tf.image.random_contrast(image, lower, upper, seed=None)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "    \n",
    "    #normalize whitening\n",
    "    # tf.image.per_image_standardization(image)\n",
    "    # Linearly scales each image in image to have mean 0 and variance 1.\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "        \n",
    "    return (final_image, image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.shuffle_batch\n",
    "# tf.train.shuffle_batch() 将队列中数据打乱后再读取出来．\n",
    "# 函数是先将队列中数据打乱，然后再从队列里读取出来，因此队列中剩下的数据也是乱序的．\n",
    "# tensors：排列的张量或词典．\n",
    "# batch_size：从队列中提取新的批量大小．\n",
    "# capacity：队列中元素的最大数量．\n",
    "# min_after_dequeue：出队后队列中元素的最小数量，用于确保元素的混合级别．\n",
    "# num_threads：线程数量．\n",
    "# seed：队列内随机乱序的种子值．\n",
    "# enqueue_many：tensors中的张量是否都是一个例子．\n",
    "# shapes：每个示例的形状．(可选项）\n",
    "# allow_smaller_final_batch：为True时，若队列中没有足够的项目，则允许最终批次更小．(可选项）\n",
    "# shared_name：如果设置，则队列将在多个会话中以给定名称共享．(可选项）\n",
    "# name：操作的名称．(可选项）\n",
    "\n",
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(i))  for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, extract_folder,'test_batch_{}.bin')]\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image,label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    # min_after_dequeue: (threads+error margin)*batch_size\n",
    "    min_after_dequeue =100\n",
    "    capacity = min_after_dequeue + 3*batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image,label], batch_size, capacity, min_after_dequeue)\n",
    "    return (example_batch,label_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    \n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return (tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.5)))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return (tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0)))\n",
    "\n",
    "    # First Conv Layer\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        conv1_kernel = truncated_normal_var(name='conv_kernel1', shape=[5,5,3,64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1,1,1,1], padding='SAME')\n",
    "        conv_bias = zero_var(name='conv1_bias', shape=[64],dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv_bias)\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "        #max pooling\n",
    "        pool1 = tf.nn.max_pool(relu_conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME', name='pool_layer1')\n",
    "        #normalization\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm1')\n",
    "        print(norm1)\n",
    "        \n",
    "    #Second Conv Layer\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        conv2_kernel = truncated_normal_var(name='conv_kernel2', shape=[5,5,64,64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1,1,1,1], padding='SAME')\n",
    "        conv2_bias = zero_var(name='conv2_bias', shape=[64],dtype=tf.float32)\n",
    "        relu_conv2 = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        #max pooling\n",
    "        pool2 = tf.nn.max_pool(relu_conv2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME', name='pool_layer2')\n",
    "        norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0,alpha=1e-3, beta=0.75, name='norm2')\n",
    "        reshaped_output = tf.reshape(norm2, [batch_size,-1])\n",
    "        reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "        print(reshaped_dim)\n",
    "        \n",
    "    #First Fully Connected Layer\n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        full1_weight1 = truncated_normal_var(name='full_mult1', shape=[reshaped_dim,384], dtype=tf.float32)\n",
    "        full1_bias1 = zero_var(name='full_bias1', shape=[384],dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output,full1_weight1),full1_bias1))\n",
    "        print(full_layer1)\n",
    "\n",
    "    #Second Fully Connected Layer\n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        full1_weight2 = truncated_normal_var(name='full_mult2', shape=[384,192], dtype=tf.float32)\n",
    "        full1_bias2 = zero_var(name='full_bias2', shape=[192],dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1,full1_weight2),full1_bias2))\n",
    "        print(full_layer2)\n",
    "        \n",
    "    #Final Fully Connected Layer\n",
    "    with tf.variable_scope('full3') as scope:\n",
    "        full1_weight3 = truncated_normal_var(name='full_mult3', shape=[192,num_targets], dtype=tf.float32)\n",
    "        full1_bias3 = zero_var(name='full_bias3', shape=[num_targets],dtype=tf.float32)\n",
    "        final_output = tf.nn.relu(tf.add(tf.matmul(full_layer2,full1_weight3),full1_bias3))\n",
    "        print(final_output)\n",
    "    \n",
    "    return (final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels =targets, logits=logits)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    return (cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss_value, generation_num):\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num, num_gens_to_wait, lr_decay, staircase=True)\n",
    "    my_optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    train_step = my_optimizer.minimize(loss_value)\n",
    "    return (train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_of_batch(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DecodeRaw:0\", shape=(?,), dtype=uint8)\n",
      "Tensor(\"Cast:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"Reshape:0\", shape=(3, 32, 32), dtype=uint8)\n",
      "Tensor(\"transpose:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "Tensor(\"Cast_1:0\", shape=(32, 32, 3), dtype=float32)\n",
      "Tensor(\"DecodeRaw_1:0\", shape=(?,), dtype=uint8)\n",
      "Tensor(\"Cast_4:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"Reshape_2:0\", shape=(3, 32, 32), dtype=uint8)\n",
      "Tensor(\"transpose_1:0\", shape=(32, 32, 3), dtype=uint8)\n",
      "Tensor(\"Cast_5:0\", shape=(32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "images, targets = input_pipeline(batch_size, train_logical=True)\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model_definition/conv1/norm1:0\", shape=(128, 12, 12, 64), dtype=float32)\n",
      "2304\n",
      "Tensor(\"model_definition/full1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "Tensor(\"model_definition/full2/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "Tensor(\"model_definition/full3/Relu:0\", shape=(128, 10), dtype=float32)\n",
      "Tensor(\"model_definition/conv1_1/norm1:0\", shape=(128, 12, 12, 64), dtype=float32)\n",
      "2304\n",
      "Tensor(\"model_definition/full1_1/Relu:0\", shape=(128, 384), dtype=float32)\n",
      "Tensor(\"model_definition/full2_1/Relu:0\", shape=(128, 192), dtype=float32)\n",
      "Tensor(\"model_definition/full3_1/Relu:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('model_definition') as scope:\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cifar_loss(model_output, targets)\n",
    "accuracy = accuracy_of_batch(test_output, test_targets)\n",
    "generation_num = tf.Variable(0, trainable=False)\n",
    "train_op = train_step(loss, generation_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 123145600851968)>,\n",
       " <Thread(Thread-5, started daemon 123145606107136)>,\n",
       " <Thread(Thread-6, started daemon 123145611362304)>,\n",
       " <Thread(Thread-7, started daemon 123145616617472)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50: Loss = 2.303088\n",
      "Generation 100: Loss = 2.302793\n",
      "Generation 150: Loss = 2.302259\n",
      "Generation 200: Loss = 2.302094\n",
      "Generation 250: Loss = 2.303295\n",
      "Generation 300: Loss = 2.303544\n",
      "Generation 350: Loss = 2.302510\n",
      "Generation 400: Loss = 2.302585\n",
      "Generation 450: Loss = 2.302585\n",
      "Generation 500: Loss = 2.302585\n",
      "--- Test Accuracy =7.81 \n",
      "Generation 550: Loss = 2.302585\n",
      "Generation 600: Loss = 2.302585\n",
      "Generation 650: Loss = 2.302585\n",
      "Generation 700: Loss = 2.302585\n",
      "Generation 750: Loss = 2.302585\n",
      "Generation 800: Loss = 2.302585\n",
      "Generation 850: Loss = 2.302585\n",
      "Generation 900: Loss = 2.302585\n",
      "Generation 950: Loss = 2.302585\n",
      "Generation 1000: Loss = 2.302585\n",
      "--- Test Accuracy =12.50 \n",
      "Generation 1050: Loss = 2.302585\n",
      "Generation 1100: Loss = 2.302585\n",
      "Generation 1150: Loss = 2.302585\n",
      "Generation 1200: Loss = 2.302585\n",
      "Generation 1250: Loss = 2.302585\n",
      "Generation 1300: Loss = 2.302585\n",
      "Generation 1350: Loss = 2.302585\n",
      "Generation 1400: Loss = 2.302585\n",
      "Generation 1450: Loss = 2.302585\n",
      "Generation 1500: Loss = 2.302585\n",
      "--- Test Accuracy =10.94 \n",
      "Generation 1550: Loss = 2.302585\n",
      "Generation 1600: Loss = 2.302585\n",
      "Generation 1650: Loss = 2.302585\n",
      "Generation 1700: Loss = 2.302585\n",
      "Generation 1750: Loss = 2.302585\n",
      "Generation 1800: Loss = 2.302585\n",
      "Generation 1850: Loss = 2.302585\n",
      "Generation 1900: Loss = 2.302585\n",
      "Generation 1950: Loss = 2.302585\n",
      "Generation 2000: Loss = 2.302585\n",
      "--- Test Accuracy =4.69 \n",
      "Generation 2050: Loss = 2.302585\n",
      "Generation 2100: Loss = 2.302585\n",
      "Generation 2150: Loss = 2.302585\n",
      "Generation 2200: Loss = 2.302585\n",
      "Generation 2250: Loss = 2.302585\n",
      "Generation 2300: Loss = 2.302585\n",
      "Generation 2350: Loss = 2.302585\n",
      "Generation 2400: Loss = 2.302585\n",
      "Generation 2450: Loss = 2.302585\n",
      "Generation 2500: Loss = 2.302585\n",
      "--- Test Accuracy =10.94 \n",
      "Generation 2550: Loss = 2.302585\n",
      "Generation 2600: Loss = 2.302585\n",
      "Generation 2650: Loss = 2.302585\n",
      "Generation 2700: Loss = 2.302585\n",
      "Generation 2750: Loss = 2.302585\n",
      "Generation 2800: Loss = 2.302585\n",
      "Generation 2850: Loss = 2.302585\n",
      "Generation 2900: Loss = 2.302585\n",
      "Generation 2950: Loss = 2.302585\n",
      "Generation 3000: Loss = 2.302585\n",
      "--- Test Accuracy =14.06 \n",
      "Generation 3050: Loss = 2.302585\n",
      "Generation 3100: Loss = 2.302585\n",
      "Generation 3150: Loss = 2.302585\n",
      "Generation 3200: Loss = 2.302585\n",
      "Generation 3250: Loss = 2.302585\n",
      "Generation 3300: Loss = 2.302585\n",
      "Generation 3350: Loss = 2.302585\n",
      "Generation 3400: Loss = 2.302585\n",
      "Generation 3450: Loss = 2.302585\n",
      "Generation 3500: Loss = 2.302585\n",
      "--- Test Accuracy =11.72 \n",
      "Generation 3550: Loss = 2.302585\n",
      "Generation 3600: Loss = 2.302585\n",
      "Generation 3650: Loss = 2.302585\n",
      "Generation 3700: Loss = 2.302585\n",
      "Generation 3750: Loss = 2.302585\n",
      "Generation 3800: Loss = 2.302585\n",
      "Generation 3850: Loss = 2.302585\n",
      "Generation 3900: Loss = 2.302585\n",
      "Generation 3950: Loss = 2.302585\n",
      "Generation 4000: Loss = 2.302585\n",
      "--- Test Accuracy =10.16 \n",
      "Generation 4050: Loss = 2.302585\n",
      "Generation 4100: Loss = 2.302585\n",
      "Generation 4150: Loss = 2.302585\n",
      "Generation 4200: Loss = 2.302585\n",
      "Generation 4250: Loss = 2.302585\n",
      "Generation 4300: Loss = 2.302585\n",
      "Generation 4350: Loss = 2.302585\n",
      "Generation 4400: Loss = 2.302585\n",
      "Generation 4450: Loss = 2.302585\n",
      "Generation 4500: Loss = 2.302585\n",
      "--- Test Accuracy =9.38 \n",
      "Generation 4550: Loss = 2.302585\n",
      "Generation 4600: Loss = 2.302585\n",
      "Generation 4650: Loss = 2.302585\n",
      "Generation 4700: Loss = 2.302585\n",
      "Generation 4750: Loss = 2.302585\n",
      "Generation 4800: Loss = 2.302585\n",
      "Generation 4850: Loss = 2.302585\n",
      "Generation 4900: Loss = 2.302585\n",
      "Generation 4950: Loss = 2.302585\n",
      "Generation 5000: Loss = 2.302585\n",
      "--- Test Accuracy =10.16 \n",
      "Generation 5050: Loss = 2.302585\n",
      "Generation 5100: Loss = 2.302585\n",
      "Generation 5150: Loss = 2.302585\n",
      "Generation 5200: Loss = 2.302585\n",
      "Generation 5250: Loss = 2.302585\n",
      "Generation 5300: Loss = 2.302585\n",
      "Generation 5350: Loss = 2.302585\n",
      "Generation 5400: Loss = 2.302585\n",
      "Generation 5450: Loss = 2.302585\n",
      "Generation 5500: Loss = 2.302585\n",
      "--- Test Accuracy =10.16 \n",
      "Generation 5550: Loss = 2.302585\n",
      "Generation 5600: Loss = 2.302585\n",
      "Generation 5650: Loss = 2.302585\n",
      "Generation 5700: Loss = 2.302585\n",
      "Generation 5750: Loss = 2.302585\n",
      "Generation 5800: Loss = 2.302585\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_accuracy = []\n",
    "for i in range(generations):\n",
    "    _, loss_value = sess.run([train_op, loss])\n",
    "    if (i+1)% output_every == 0:\n",
    "        train_loss.append(loss_value)\n",
    "        output = 'Generation {}: Loss = {:5f}'.format((i+1), loss_value)\n",
    "        print(output)\n",
    "    if(i+1)%eval_every == 0:\n",
    "        [temp_accuracy]=sess.run([accuracy])\n",
    "        test_accuracy.append(temp_accuracy)\n",
    "        acc_output = '--- Test Accuracy ={:.2f}% '.format(100.*temp_accuracy)\n",
    "        print(acc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
